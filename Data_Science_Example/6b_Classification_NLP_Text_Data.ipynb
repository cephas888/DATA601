{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b966353",
   "metadata": {},
   "source": [
    "Classification of Movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c3d85d-f164-4d87-a118-63c819efe375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec8f757c-e229-49bc-ac64-c728e62b41ba",
   "metadata": {},
   "source": [
    "### Problem Statement\r\n",
    "Return to Table of Contents\r\n",
    "\r\n",
    "In DATA data there are about 1500 cases that use the generic \"Injury\" code under the \"Injury/Illness\" column or feature. We want to understand if a classification model can be used to better label these reports with a more relevant injury/illness code. To do this we can use NLP and Supervised Classification model to explore what would be better injury/illness codes (i.e., classes or lables).\r\n",
    "\r\n",
    "This notebook will focus on applying the classification algorithmand classifying those records labeled as \"INJURY\" and explore any other potential class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12899607-5f9a-44e9-a904-b3af4c7c41a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import wordnet, WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import pos_tag # For parts of speech\n",
    "from nltk import word_tokenize # To create tokens\n",
    "from nltk.corpus import wordnet, stopwords # For stop words\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.metrics import r2_score, accuracy_score, mean_absolute_error, confusion_matrix, ConfusionMatrixDisplay, classification_report, balanced_accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75782061-fbfb-4b06-97a4-306a47e720fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None) # PD has a limit of 50 characters.  This takes out the limit and uses the full text.\n",
    "pd.options.display.float_format = \"{:.4f}\".format # Pandas displays float numbers as 4 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b586c4f6-d399-4cc2-8621-b219d5acf1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default color cycle for MatPlotLib Plots\n",
    "#plt.rcParams['axes.prop_cycle'] = plt.cycler(color=['b', 'r', 'g']) # Specifies specific color cycling (https://matplotlib.org/stable/gallery/color/named_colors.html)\n",
    "#plt.style.available # Available Color Styles\n",
    "plt.style.use('tableau-colorblind10') # Defining a specific color style to use.  Tableu-Colorblind\n",
    "#plt.style.use('seaborn-colorblind') # Defining a specific color style to use. Seaborn-colorblind\n",
    "\n",
    "#colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] # Extract Colors being defined in the plt.style.use                       \n",
    "#print('\\n'.join(color for color in colors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794f75eb-4958-4b9b-b367-5be4d4f94acc",
   "metadata": {},
   "source": [
    "# TEXT NORMALIZATION FUNCTIONS\r\n",
    "Return to Table of Contents\r\n",
    "\r\n",
    "Note of caution. If we are using the normalization functions to do some query ranking like calcualtion (e.g., similarity ranking) and apply the same function that was applied to the target cleaned text and input query text. In the case for classification we will not use the text normalization function, however, when vectorizing we may call stopwords and we may inadvertently introduce new stopwords (more on this below).\r\n",
    "\r\n",
    "For now, recall the list of stopwords and text normalization functions that we use in the previous step and Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fc6686-0c06-49ed-85a3-a333ceb9862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords used in this DATASET.\n",
    "stopwords_to_add = ['area','building','employee','employees','facility','personnel','work','worker','workers', \n",
    "                    'na', 'n/a']\n",
    "remove_as_stopword = ['no']\n",
    "#stopwords.words('english') # Default Stopwords from NLTK\n",
    "stopwords_english = list(filter(lambda w: w not in remove_as_stopword, stopwords.words('english'))) \n",
    "stopwords_custom = stopwords_english + stopwords_to_add\n",
    "print(stopwords_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0468e52a-811c-4688-b191-2213b3ccfb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalization(text, word_reduction_method):\n",
    "    text = str(text) # Convert narrative to string.\n",
    "    df = pd.DataFrame({'': [text]}) # Converts narrative to a dataframe format use replace functions.\n",
    "    df[''] = df[''].str.lower() # Covert narrative to lower case.\n",
    "    df[''] = df[''].str.replace(\"\\d+\", \" \") # Remove numbers\n",
    "    df[''] = df[''].str.replace(\"[^\\w\\s]\", \" \") # Remove special characters\n",
    "    df[''] = df[''].str.replace(\"_\", \" \") # Remove underscores characters\n",
    "    df[''] = df[''].str.replace('\\s+', ' ', regex=True) # Replace multiple spaces with single\n",
    "    text = str(df[0:1]) # Extracts narrative from dataframe.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # Tokenizer.\n",
    "    tokens = tokenizer.tokenize(text) # Tokenize words.\n",
    "    filtered_words = [w for w in tokens if len(w) > 1 if not w in stopwords_custom] # Note for NRC remove words of 1 letter only (e.g., don't remove TS and other accronyms of 1 letter). Can increase to higher value as needed.\n",
    "    if word_reduction_method == 'Lemmatization':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        reduced_words=[lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in filtered_words] # Lemmatization.  The second argument is the POS tag.\n",
    "    if word_reduction_method == 'Stemming':\n",
    "        stemmer = PorterStemmer() # Stemming also could make the word unreadable but is faster than lemmatization.\n",
    "        reduced_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    return \" \".join(reduced_words) # Join words with space.\n",
    "\n",
    "def get_wordnet_pos(word): # Reference: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/#wordnetlemmatizer\n",
    "    #\"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f828c0d8-925e-4689-927c-1525f9901efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA LOADING: DF Cleaned Data\n",
    "Return to Table of Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceacbcc-6950-4cd4-ae78-ffdd23463baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataframe.\n",
    "\n",
    "df_DF = pd.read_csv('.\\output_data\\df_DF_clean_ML_All.csv', encoding = \"utf-8-sig\", #parse_dates=['Occurrence Date', 'Reporting Date', 'Final Date'],\n",
    "                      keep_default_na=False,\n",
    "                      na_values=['', '-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A','N/A', '#NA', 'NULL', 'NaN', '-NaN', 'nan', '-nan']) # Removed 'NA' from list as NA is used for NNSA.\n",
    "\n",
    "# Encoding \"cp1252\" or \"utf-8-sig\" used so that Excel does not create special characters. Standard Python is utf-8.\n",
    "# See reference for explanation https://stackoverflow.com/questions/57061645/why-is-%C3%82-printed-in-front-of-%C2%B1-when-code-is-run\n",
    "\n",
    "df_DF.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af8115-b6ad-4e2e-a663-658fb95a58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DF.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8285a9-36bd-4be7-a3df-3baa89f8da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column selection for selecting columsn in loops used in the data cleaning, visualization and model functions below.\n",
    "dfcolumns = list(df_DF.columns.values)\n",
    "dfcolumns_index = pd.DataFrame(dfcolumns, columns=['column'])\n",
    "pd.set_option('display.max_rows', None) # Uncomment to see all the columsn in the DF.\n",
    "dfcolumns_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84dd23-8ccd-49f0-8031-e148ddc54f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_DF.shape)\n",
    "df_DF.info()\n",
    "# Note that the range index and the total number of rows match.\n",
    "# There may be some columns here that we may not need but for the excercise we will leave them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c858c8-8d03-4143-b9af-cfa83eaf3fc1",
   "metadata": {},
   "source": [
    "# EDA Pre-Classification\n",
    "Return to Table of Contents\n",
    "\n",
    "At this point we want to evaluate the data and identify any potential issues (e.g., data balance). Many of the features identified in the columns could in theory be used to classify the DF reports that is if the report information includes enough information for such classification. We had previously identified issues with data balance.\n",
    "\n",
    "In our problem statement we want to reclassify the reports labeled as \"INJURY\" into something more specific. Given the data we could train a classification model to use the reports to classify the \"Body parts\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a145823d-c052-4532-92de-026162fc13ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DF.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc81f06-3573-46ab-ae28-562a4be6094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of \"ORG\" unique values: {len(df_DF[\"ORG\"].unique())}')\n",
    "df_DF['ORG'].value_counts(dropna = False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0fbdbc-e8ad-44c0-bdf2-be7e998633bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of \"Site\" unique values: {len(df_DF[\"Site\"].unique())}')\n",
    "df_DF['Site'].value_counts(dropna = False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292593d3-e9ea-43a1-a35a-4a022ddb5fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of \"Activity\" unique values: {len(df_DF[\"Activity\"].unique())}')\n",
    "df_DF['Activity'].value_counts(dropna = False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe00f85-9c8d-4dec-954c-01c0d60461f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of \"Body Part\" unique values: {len(df_DF[\"Body Part\"].unique())}')\n",
    "df_DF['Body Part'].value_counts(dropna = False).head(5)\n",
    "# Some of the \"body parts\" will be highly correlated to some \"injury/illness\" \n",
    "# Like \"Ear(s)\" body part and \"Hearing Loss\" or \"Hearing Impairment\" injury/illness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd46e7-693d-4540-a19f-ed4b765108de",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_injury_illness_counts = df_DF['Injury / Illness'].value_counts(dropna = False).rename_axis('unique_values').reset_index(name='counts')\n",
    "DF_injury_illness_counts#.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c36e711-4390-4593-8225-4863d65d1708",
   "metadata": {},
   "source": [
    "Notes: In this df_DF dataset that we have we have a total of over 20K reports and a total of 97 classes/lables, some of which are very well represented while others are not well represented (e.g., data imbalance or data balance issue). For example, there are over XXXX reports that are labeled as \"Strain\" while a few labels (e.g., electrocution, snake bite, hepatitis, etc.) only have one report. This is an example of data balance issue or imbalanced dataset. Because of this, an algorithm will have challenges trying to apply these low representative labels from the training data. Not only this but we will not be able to evaluate performance.\n",
    "\n",
    "To address this there may be a few approaches that we can take:\n",
    "\n",
    "Get more representative data for those reports that do not have enough records.\n",
    "Undersampling/oversampling approaches\n",
    "Obtain more data representative of these codes.\n",
    "Create synthetic data: With the help of a subject matter expert, create synthetic records with words and terms expected for that label/class.\n",
    "Augment the data (i.e., from other similar datasets): For example, we could try to obtain similar data from BLS/CDC/NIOSH and itegrate with this one. Integration would need to have some form of processing step to make the datasets compatible.\n",
    "Undersampling: Remove labels that do not meet a threshold (e.g., minimum of 50 records). This will mean that we will loose those labels below the threshold.\n",
    "Oversampling:\n",
    "Simplest approach would be to repeat the records a number of times until it meats a minimum threshold.\n",
    "Synthetic Minority Over-sampling TEchnique (SMOTE)\n",
    "Use the data as is without modifying with the acknowledgement that these classes with low numbers will probably not be reliable and we will not be able to evalaute its performance.\n",
    "Develop a curated standard used for machine learning approaches which addresses known challenges (e.g., data balance, data bias, etc.)\n",
    "Use combination of multiple approaches above.\n",
    "Manually divide the data especially for those that have small number of values with the acknowledgement that there is not enough data and model may not perform well in this classes.\n",
    "In our case we would ideally also curate a sample of legitimate \"INJURY\" reports and use that for the training data of our model.\n",
    "In any case those labels that have a very small number of records have a low frequency of occurring and most probably the reports that we want to reclassify there is also a low probability that they would fall within these labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0373c61-13dd-4470-a223-7f759cb228c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO EXPORT AS CSV\n",
    "DF_injury_illness_counts.to_csv (r'.\\output_data\\DF_injury_illness_counts.csv', \n",
    "                                    encoding='utf-8-sig', index = False, header=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f08537-c0e3-42f7-934a-bbe66831e703",
   "metadata": {},
   "source": [
    "# Address Data Balance\n",
    "Return to Table of Contents\n",
    "\n",
    "As discussed above, there are multiple approaches that we can take each with its own advantages and disadvantages. For this example, we will use any class label that has at least 60 records.\n",
    "\n",
    "This approach will develop a new dataframe that will be used for training/testing that has at least 60 records for each class up to 300. To accomplish this there are many scripts that we can develop to filter the data. We could focus on long narratives, do random sampling, manually select data, etc.\n",
    "\n",
    "This approach does the following:\n",
    "\n",
    "Random sampling for classes/lables that have more than 300 records and selects all records for those that have less.\n",
    "Drops all records with a value counts of label/class less than 60\n",
    "Removes the records with the generic \"Injury\" label as the purpose of the excercise is that once we have a model trained we want to assign new labels to the records that have the \"Injury\" label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328a8dab-4ca3-420b-b117-a07eec98cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Sampling to address data balance.\n",
    "# Random sample for classes that have 300 or more and keeps all that have less.\n",
    "df_DF_filtered = df_DF.groupby('Injury / Illness').apply(lambda s: s.sample(min(len(s), \n",
    "                                                                                      500))).copy().reset_index(drop = True)\n",
    "# Filters out classes that have less than 50 records if any.\n",
    "df_DF_filtered = df_DF_filtered.groupby('Injury / Illness').filter(lambda x: len(x) > 60).reset_index(drop = True)\n",
    "\n",
    "# We also don't want to have records with the generic \"INJURY\" as the point of the excercise is to find a better class/lable.\n",
    "df_DF_filtered = df_DF_filtered[df_DF_filtered['Injury / Illness'] != 'INJURY'].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ddb149-6b0c-4ee8-92d5-5c4e4baff42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_DF_filtered.shape)\n",
    "df_DF_filtered.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1adddc-c2e7-4a5f-a276-256ac103ae01",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "We can see that our df_DF_filtered dataframe now only has value counts of labels between 300 records and 60.\n",
    "We will use this new dataframe for training/testing our model.\n",
    "We could probably spend many hours just discussing is this is or it is not a correct approach.\n",
    "There are some classes (e.g., those that have less than 60 records) that we will not be represented and the model will not assign.\n",
    "Labels like \"HEARING IMPAIRMENT\" had less than 60 records while \"HEARING LOSS\" had more. This will cause our model that any hearing related issue will potentially assign \"HEARING LOSS\" we need to recognize the limitation of filtering classes out in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293ba93b-1b48-4403-a97f-c9b56e222646",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_DF_filtered.shape)\n",
    "df_DF_filtered['Injury / Illness'].value_counts().rename_axis('unique_values').reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e686723-3fe2-4b25-8363-96a84ee739a2",
   "metadata": {},
   "source": [
    "# Bag of Words and Classification Models\r\n",
    "Return to Table of Contents\r\n",
    "\r\n",
    "This section discusses the Bag of Words (BoW) model matrix and classification model (training, and testing).\r\n",
    "\r\n",
    "The classification model used in this notebook is a logistic regression which is one fo the simplest more explainable classification models. Using a different model will follow the same process. There are many methods and algorithms for classification (https://www.geeksforgeeks.org/multiclass-classification-using-scikit-learn/).\r\n",
    "\r\n",
    "The approach initially used here is that from BLS Alexander Measures using a logistic regression. Note that these were the early approaches and U.S. Bureau of Labor Statistics (BLS) and U.S. the Center for Disease Control and Prevention (CDC) National Institute for Occupational Safety and Health (NIOSH) have published newer methods which may be more accurate. BLS and NIOSH have been using ML classification algorithms for more than a decade to classify worker injury data of the U.S. industry. Their data is from the Occupational Injury and Illness Classification System (OIICS)\r\n",
    "\r\n",
    "References:\r\n",
    "\r\n",
    "NIOSH CDC NASA AI MSHA Data Autocoding Competition Github (https://github.com/NASA-Tournament-Lab/CDC-NLP-Occ-Injury-Coding): Solutions use deep learning algorithms to classify the data.\r\n",
    "BLS Autocoding with Deep Neural Networks (https://www.bls.gov/iif/automated-coding/deep-neural-networks.pdf)\r\n",
    "BLS Alexander Measures Autocoding Github (https://github.com/ameasure/autocoding-class): Solution sses Logistic Regression model to classify the data.\r\n",
    "BLS MSHA Autocoding: https://www.bls.gov/iif/automated-coding.htm\r\n",
    "MSHA Data and Reports: https://www.msha.gov/data-and-reports\r\n",
    "NIOSH AI MSHA Data Autocoding Competition https://archive.cdc.gov/www_cdc_gov/niosh/updates/upd-02-26-20.html\r\n",
    "Other References:\r\n",
    "\r\n",
    "https://towardsdatascience.com/how-to-balance-a-dataset-in-python-36dff9d12704\r\n",
    "https://towardsdatascience.com/essential-guide-to-multi-class-and-multi-output-algorithms-in-python-3041fea55214"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fd91bf-7bbe-4a74-83d5-ff211e038726",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW) Model Matrix\n",
    "Return to Table of Contents\n",
    "\n",
    "The Bag of Words (BoW) model is a vector representation of each token (e.g., words, phrases, sentences, etc.) in the text of each record. This matrix is then used to perform calculations in this case to calculate which class or label a text belongs to.\n",
    "\n",
    "References:\n",
    "\n",
    "SK-Learn Bag of Words: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#bags-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e981a349-e1fc-43a0-96ca-c58c3f928fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating BOW for the Target DataFrame with selected Vectorizer TFIDF\n",
    "# Note that CountVectorizer is another vectorization function.\n",
    "# The below function defines an instance of the vectorizer with its specific parameters.\n",
    "# Note that the ngram, max_df and min_df will have a large impact on the size of the matrix.\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=True, \n",
    "                             analyzer='word',\n",
    "                             stop_words=stopwords_custom, \n",
    "                             # Ensures I am applying the same stopwords here and in text normalization function.\n",
    "                             ngram_range=(1, 2), # Considers 1-grams (i.e., single words) and 2-grams (i.e., two words) \n",
    "                             max_df = 0.95, \n",
    "                             min_df = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa37cd-d123-4105-91fe-c1fe55e48082",
   "metadata": {},
   "source": [
    "The classification algorithm can be applied to the vectorization of different texts in the dataframe. In this notebook we have created/divided a version of the text that is lemmatized and another version that is stemmed from the combined text fields of df_DF data.\n",
    "\n",
    "As a data scientist you will need to decide what word reduction method (if any) works best for your application. Note that there may be many different approaches to develop a model. The BLS and NIOSH references above also provide examples of different ways to solve the same problem.\n",
    "\n",
    "It is important to note that NLP are high dimensional problems where each token is a dimension. In many cases it is recommended to use dimensionalit reduction methods. See more on the Dimensionality Reduction Section.\n",
    "\n",
    "Vectorization function references:\n",
    "\n",
    "SK Learn TFIDF Vectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "SK Learn Count Vectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "For max_df and min_df explanation: https://stackoverflow.com/questions/27697766/understanding-min-df-and-max-df-in-scikit-countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a529916a-588b-4342-a708-663317316aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW Array creates an array of the vectors for each token.\n",
    "# The vectors for each token are represented as a TFIDF value.\n",
    "# Note that if we wanted to use the lemmatized version of teh text just change the column name.\n",
    "bow_array = vectorizer.fit_transform(df_DF_filtered['norm_text_stem']).toarray()\n",
    "#bow_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c7a29d-cd2d-4c7f-8b32-bf23e9f01ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.get_feature_names_out() # Extracts the token names.\n",
    "df_bow = pd.DataFrame(bow_array, columns = features) # Converts the BoW array to a dataframe with token names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d5524d-f3c1-4a82-b364-225df1f12a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The BoW Matrix is also called sparse matrix\n",
    "print(df_bow.shape)\n",
    "df_bow.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84e8509-4b87-456c-ae68-6b60105296d8",
   "metadata": {},
   "source": [
    "# Classification Process\n",
    "Return to Table of Contents\n",
    "\n",
    "Recall from Notebook_0, when developing and evaluating a supervised ML model (e.g., classification), it involves the following general steps:\n",
    "\n",
    "Defining dependent variable or variables to be predicted (y)\n",
    "Defining independent variable or variables (x) that will be used to predict y\n",
    "Scaling/normalization if needed\n",
    "Train/Test Split (Typically 80/20)\n",
    "Fitting the training data to the model\n",
    "Calculate predictions using the testing data\n",
    "Evaluate Model Performance (e.g., Calculating metrics, predicting, using the model, etc.)\n",
    "Deploy and use the model.\n",
    "Data considerations and issues that may affect supervised machine learning:\n",
    "\n",
    "Data balance (e.g., are all classes represented). Null accurac may be a worthwhile metric.\n",
    "Amount of data. Evaluate if there is enough data for dividing the dataset into training/testing (typically 80/20).\n",
    "Data bias: any biases in the data most probably, knowingly or unknowingly, will also show in the outputs of the model.\n",
    "Explore the need for using synthetic/augmented data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fbcdd7-c1ae-4a66-9e79-19a6841f9894",
   "metadata": {},
   "source": [
    "# Step 1: Defining Dependent Variables\n",
    "Return to Table of Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf97875b-95c2-406d-81d3-bc95d8a48f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_DF_filtered.loc[:,'Injury / Illness'] # Variable to be predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5bf35-5180-4b5f-9785-7a33b09c910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f380e2-2dd7-4f6a-b83b-a2f499fd468e",
   "metadata": {},
   "source": [
    "# Step 2: Defining Independent Variables\n",
    "Return to Table of Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefbcf5c-6d4d-4883-aafc-6e440cec814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_DF_filtered.loc[:,'norm_text_stem'] # The features we want to use for predicting is the tokens from the narratives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650f10d0-c94e-4108-a4e1-9066bd128602",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad3c1a-e2d5-497e-a601-89a6f363c20d",
   "metadata": {},
   "source": [
    "# Step 3: Scaling/Normalization (if needed)\n",
    "Return to Table of Contents\n",
    "\n",
    "TFIDF vectors are already normalized and there is no need to normalize data in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888638e2-0d3a-475b-be93-9faaa085a4b5",
   "metadata": {},
   "source": [
    "# Step 4: Train/Test Split\n",
    "Return to Table of Contents\n",
    "\n",
    "We use the scikit-learn train_test_split function (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2880208d-3e10-4f03-9bf0-f9b4bcf264d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the sampled data into 80 for Training and 20 for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, \n",
    "                                                    stratify = y, # Stratified sampling attempts to balance classes.\n",
    "                                                    test_size=0.2, # 80/20 for training/testing\n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818db470-b252-488a-93e5-303e627a79ae",
   "metadata": {},
   "source": [
    "Stratified sampling tries to balance the training data and maintain the relative class frequencies is approximately preserved in each train and validation fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b1b71e-db51-4abd-b4ab-69a86a10d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.sample(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3708a2ed-c8d5-4abe-880c-f90d7d4c259b",
   "metadata": {},
   "source": [
    "# Step 4a: Vectorization\n",
    "Return to Table of Contents\n",
    "\n",
    "Because this is NLP Classification there is an extra step to vectorize the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c588647-1333-485b-94cf-c693d61ffa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the training text data to a Bag of Words Array Matrix of the vectors of previously defined tokens.\n",
    "X_train = vectorizer.transform(X_train).toarray()\n",
    "\n",
    "# Note that we could have also recreated a new BoW matrix by using the fit_transform here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c54ab3-6e22-48d9-ba0d-8f8d33a8b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total lenght of the data {len(y)}.\") # Total number of values in df_DF_filtered dataframe\n",
    "print(f\"Training Lenght of the data {len(y_train)}.\") # This should be approximately 80% of the data.\n",
    "print(f\"Testing Lenght of the data {len(y_test)}.\") # This should be approximately 20% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0062cfa9-f6a2-462d-88a5-2414f2405a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_DF_filtered.shape) # Shape of the df_DF_filtered dataframe.\n",
    "print(len(X_train), len(X_train[1])) # Shape of X_train\n",
    "X_train # Sample of X_train (i.e., sparse matrix of training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9058e193-050a-4e95-8acf-5b964d9ca6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the testing data to the BoW Array of the Training data\n",
    "X_test = vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9038ff1c-5ce1-4557-b060-789bf548feee",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE If you erroneously apply the \".fit_transform\" you may get an error in later parts of the process. fit_transform will recalculate new tokens and BoW matrix based on the test data and cause Train and Test data BoW matrices to have different shapes. The Transform creates the vectors for the Testing data based on the vectors of the Training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c60b6-fcf8-4a39-9786-c377ea518cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_DF_filtered.shape) # Shape of the df_DF_filtered dataframe.\n",
    "print(len(X_test), len(X_test[1])) # Shape of X_train\n",
    "X_test # Sample of X_train (i.e., sparse matrix of training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406a20e3-7a55-461e-97b6-c60439595f68",
   "metadata": {},
   "source": [
    "# Step 5: Fitting the training data to the model\n",
    "Return to Table of Contents\n",
    "\n",
    "In this step we will fit the training data to the model. There are many classification algorithms that can be used from Logistic Regression, Support Vector Machines (SVM), Naive-Bayes, Random Forests, and other classifiers. Typically there will be a process of comparing performance of various algorithm before selecting and deployment.\n",
    "\n",
    "Documentation References:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/svm.html\n",
    "Multi-Class Classification metrics: Vary from those of binary classification and have to make adjustments in the parameters when of each of the metric evaluation functions.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "Confusion Matrix:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb5d885-8713-4b7d-90f2-4e10a62d31d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines Classifier (clf) Algorithms\n",
    "# Applying other algorithms would have a similar script by replacing the appropriate CLF.\n",
    "\n",
    "#clf = LogisticRegression(C=0.1, solver='liblinear', multi_class='auto', class_weight='balanced', random_state = 0)\n",
    "clf = RandomForestClassifier(max_depth = None,  class_weight = 'balanced', random_state=0) \n",
    "#clf = MultinomialNB() \n",
    "#clf = svm.SVC(C = 1.0, class_weight = 'balanced', break_ties = True)\n",
    "\n",
    "# Note that some algorithms may take longer to train/fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396efd3-c464-4619-b50b-1016cc753ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fits training data to CLF algorithm.\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb586fb-60dc-4d72-9708-38310174d1fb",
   "metadata": {},
   "source": [
    "# Step 6: Calculate predictions using the testing data\n",
    "Return to Table of Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1fffa1-264a-4a00-abe5-4a52f80f6af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Different CLF algorihtms may take longer to calculate predictions. \n",
    "y_predict = clf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "print('Confusion Matrix : \\n', cm)\n",
    "total1=sum(sum(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3613cb58-18c0-4da9-968e-723617d59a4b",
   "metadata": {},
   "source": [
    "# Step 7: Evaluating Model Performance\n",
    "Return to Table of Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05bded-9bcb-455c-8b7a-dcd4d17016fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "disp.plot(ax=ax)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show();\n",
    "# Diagonal shows the Predicted matches the Label from the Test data and hence predicted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e36538-5c41-4929-b0ea-5e52bed35a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of targets is needed to calculate some metrics.\n",
    "target_name_list = sorted(df_DF_filtered['Injury / Illness'].unique().tolist())\n",
    "print(target_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4a07c1-4b7c-4b2a-b084-fa93610d99b3",
   "metadata": {},
   "source": [
    "Accuracy Score Computes the accuracy, either the fraction (default) or the count (normalize=False) of correct predictions.\n",
    "\n",
    "Documentation References:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f521184e-d06f-4e5b-bd42-eecdf39d956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_predict) # Accuracy score is how many were predicted correctly vs. predicted incorrectly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6637f027-8757-424a-ae12-e04dd5ad1ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_test, y_predict) # Deals with imbalanced datasets. Is the average recall for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd0e5c6-473a-44da-91df-d5b8add9af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing of these two lists can be used to see if there is any class that appears in the Training data and \n",
    "# not in the Test data and vice versa. \n",
    "# If there are any classes identified that do not occur in one of the training/test/predicted datasets \n",
    "# it is probably caused by low number of records in the class and split not accounting for such low number.\n",
    "\n",
    "unique_train_list = sorted(y_train.unique())\n",
    "unique_test_list = sorted(y_test.unique())\n",
    "unique_pred_list = sorted(set(list(y_predict)))\n",
    "#print(f'Unique Train Classes: {unique_train_list}')\n",
    "#print(f'Unique Test Classes: {unique_test_list}')\n",
    "#print(f'Unique Predicted Classes: {unique_pred_list}')\n",
    "print(f'Classes found in Training or Test data but not both: {set(unique_train_list) - set(unique_test_list)}')\n",
    "print(f'Classes found in Test or Predicted but not both: {set(unique_pred_list) - set(unique_test_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5b0d6-a591-4fca-9d86-23462186ba4a",
   "metadata": {},
   "source": [
    "Precision Score The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The best value is 1 and the worst value is 0.\n",
    "\n",
    "The average parameter has the following options: ‘micro’, ‘macro’, ‘samples’, ‘weighted’, None or the default = ’binary’. This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data.\n",
    "\n",
    "'binary': Only report results for the class specified by pos_label. This is applicable only if targets (y_{true,pred}) are binary.\n",
    "'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "'macro': Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "'samples': Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score).\n",
    "Documentation References:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77199f3d-dbdf-47dd-958c-40a91d9a5b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_score(y_test, y_predict, labels = target_name_list, average='micro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9123c29-63b3-44db-b8e5-7354337e5684",
   "metadata": {},
   "source": [
    "Recall The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "The best value is 1 and the worst value is 0.\n",
    "\n",
    "Documentation References:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a5c0e2-0f32-4450-b70d-030f4661409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_predict, average='micro')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b59a04-3f0f-4c00-aba3-3981c0b7f3b2",
   "metadata": {},
   "source": [
    "F1 Score F1 score can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The average parameter has the same options as the Precision above.\n",
    "\n",
    "Documentation References: - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f39a74-5082-48cd-ac2f-610a3444ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y_test, y_predict, labels = target_name_list, average='micro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b83b16-6049-44f4-bf4a-51ad0c7e608b",
   "metadata": {},
   "source": [
    "Classification Report Provides the class model classification metrics for each class as well as for the full dataset. It also provides the number of records or data points used for testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb962dc-686a-4713-bd80-bac7c579ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predict, labels = target_name_list, zero_division = 0))\n",
    "# Observations:\n",
    "# Classes that had a low number of records tended to have low metrics. \n",
    "# Although low metrics can also occur in the classes with high number of records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a285c6-bbaf-49f4-afa2-2b257eec97cb",
   "metadata": {},
   "source": [
    "Note that there are many \"Injury/Illness\" labels and classes that are either very generic (e.g., \"other...\", \"multiple...\") or may have overlap/relation between definition.\n",
    "\n",
    "Having more data on these labels/classes that would benefit in improving model overall performance.\n",
    "\n",
    "# Step 8: Deploy and use the Model\n",
    "Return to Table of Contents\n",
    "\n",
    "Now that we have a model we can use this to assign, classify, or predict a better class to the generic \"INJURY\" reports.\n",
    "\n",
    "First we want to filter those reports that have the \"INJURY\" class applied in the \"Injury / Illness\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853ce815-066d-4828-938c-0772aeef2125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DF_INJURY = df_DF[df_DF['Injury / Illness'] == 'INJURY'].reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a52e655-1fa0-49e2-a0b0-7c9486d97372",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_DF_INJURY.shape)\n",
    "df_DF_INJURY['Injury / Illness'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b374c-e383-4533-876a-c9abf136a6ba",
   "metadata": {},
   "source": [
    "Now that we have the data filtered, we want to use our classifier to predict which would have been a potential better class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427cd0b-02ec-4b5f-b309-6a1583e3d2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_INJURY = df_DF_INJURY.loc[:,'norm_text_stem'] # The features we want to use for predicting is the tokens from the narratives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91d6de7-c124-42ba-ac20-782134d5941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_INJURY.sample(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f4746c-53f5-4160-a33d-7f34cd6f3239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the INJURY data to the BoW Array of the Training data\n",
    "x_INJURY = vectorizer.transform(x_INJURY).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1bafcc-9732-4655-a8b1-c0edee5754e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_test), len(X_test[1])) # Shape of X_train\n",
    "print(len(x_INJURY), len(x_INJURY[1])) # Shape of X_INJURY. The shape of [1] should match the Training data.\n",
    "x_INJURY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450ad57e-66c6-4acc-966e-11fe9f7185ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Different CLF algorihtms may take longer to calculate predictions. \n",
    "y_predict_INJURY = clf.predict(x_INJURY)  # Predicted values for our Injury Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092bbf5a-3ce6-4c42-9d01-eb46728a07b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_predict_INJURY)) # This should match our number of Injury reports.\n",
    "y_predict_INJURY[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d4e72e-c9ab-4d69-ad5c-59d8cc4c22db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with the predicted values.\n",
    "df_DF_INJURY['Predicted Injury/Illness'] = y_predict_INJURY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d967bf-516b-43e0-bb95-1168ca5561a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can extract the probability of the prediction.\n",
    "# Note that this is a probability of all labels/classes being predicted.\n",
    "y_predict_INJURY_prob = clf.predict_proba(x_INJURY)\n",
    "df_DF_INJURY['Prediction Probability'] = y_predict_INJURY_prob.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e5867f-bc0e-44e2-b5d0-256b703557bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DF_INJURY['Predicted Injury/Illness'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05913a2-0bbb-4469-8cf1-dfb989bce701",
   "metadata": {},
   "source": [
    "Let's explore some of the \"Injury / Illness\" that should be obvious for both a person and a classification model. The followin classes also do not seem to have an overlap with other labels/classes.\n",
    "\n",
    "INSECT STING: Should have terms such as insect, sting, insect names, etc.\n",
    "HEARING LOSS: Should have the terms such as hearing, noise, dB, hearing protection, audio, etc.\n",
    "LOSS OF CONSCIOUSNESS: Should have terms such as unconscious, pass out, faint, etc.\n",
    "COVID-19: Should have terms such as virus, COVID, quarintine, etc.\n",
    "Could use the probability to assign with a ML Classification Model if a high probability and those with low probability requiring manual review of a SME.\n",
    "\n",
    "Recall that some of this will be legitimate generic \"Injury\" reports. Ideally we would also include in our model's training data examples of what these generic \"Injury\" reports look like so that the model can make a decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3333d896-450b-4e44-83e8-44d47b84695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DF_INJURY[df_DF_INJURY['Predicted Injury/Illness'] == 'INSECT STING']\\\n",
    "[['ID', 'COMBINED_NARRATIVES', 'Injury / Illness', 'Predicted Injury/Illness', 'Prediction Probability']]\\\n",
    ".sort_values('Prediction Probability', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef4e66-a803-4a80-a728-77260b9f8655",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DF_INJURY[df_DF_INJURY['Predicted Injury/Illness'] == 'COVID-19']\\\n",
    "[['ID', 'COMBINED_NARRATIVES', 'Injury / Illness', 'Predicted Injury/Illness', 'Prediction Probability']]\\\n",
    ".sort_values('Prediction Probability', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732c59b8-b3bb-440a-9d06-fb6f0ba3cf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DF_INJURY[df_DF_INJURY['Predicted Injury/Illness'] == 'HEARING LOSS']\\\n",
    "[['ID', 'COMBINED_NARRATIVES', 'Injury / Illness', 'Predicted Injury/Illness', 'Prediction Probability']]\\\n",
    ".sort_values('Prediction Probability', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d18627d-4026-49d5-8b2f-c2c5bba5e980",
   "metadata": {},
   "source": [
    "# RESULTS EXPORT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5f0b8-769f-4ca9-8cb5-19f30b76de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO EXPORT AS CSV\n",
    "df_DF_INJURY.to_csv (r'.\\output_data\\df_DF_INJURY.csv',\n",
    "                        encoding='utf-8-sig', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d3b70e-d6f2-4e43-83e5-5f9e4cc768f5",
   "metadata": {},
   "source": [
    "# Using the Classifier as Recommendation System\n",
    "Return to Table of Contents\n",
    "\n",
    "A model that recommends or even fully automates classification of categorical values given the description text is possible. This system could be developed within df_DF or any system (e.g., OTHER to predict keywords). This would minimize human errors (e.g., missclassification) or even resolve issues like the above where the best label class is not appliedfor whatever the reason (e.g., staff not familiar with all classes, time constraints, lack of training, interpretation, laziness, etc.). This will also make the process more efficient while at the same time increasing data quality. However, this would require a few things:\n",
    "\n",
    "Good quality training data, potentially a curated data set\n",
    "Willingness from EHSS\n",
    "The example below uses an input text box where we can submit a text description (e.g., df_DF combined text) and it will recommend what are the best \"INJURY /ILLNESS\" classes given the classifier model training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da38e724-1226-452f-99e1-41bb41804a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to input custom narrative.\n",
    "text_to_predict = pd.Series(input('Input text to predict df_DF \"INJURY/ILLNESS\" Category'))\n",
    "# Normalizes input text using stemming and converts to Pandas series which is needed as input to the classification model.\n",
    "text_to_predict = pd.Series(text_normalization(text = text_to_predict, word_reduction_method = \"Stemming\"))\n",
    "\n",
    "# Tranform the input narrative into the BoW model vector array\n",
    "X_input = vectorizer.transform(text_to_predict).toarray()\n",
    "# Predicts the OIICS category given the input text vectors\n",
    "y_predict_input = clf.predict(X_input)\n",
    "# Probability of prediction. Because this is multiclass problem the probability of the assigned can be lower than 0.5.\n",
    "y_predict_input_prob = sorted(clf.predict_proba(X_input)[0], reverse = True)[0]\n",
    "\n",
    "df_prediction_probabilities = pd.DataFrame()\n",
    "df_prediction_probabilities['CLASS'] = y.unique() # Class labels\n",
    "df_prediction_probabilities['PROBABILITIES'] = clf.predict_proba(X_input)[0] # Probabilities of all classes.\n",
    "\n",
    "# Prints the Category Number and the OIICS Event Description.\n",
    "print(f\"\\nPredicted Text INJURY/ILLNESS is: {y_predict_input[0]}\")\n",
    "print(f\"Probability of prediction is: {y_predict_input_prob}\")\n",
    "\n",
    "display(df_prediction_probabilities.sort_values('PROBABILITIES', ascending = False))\n",
    "print(f\"Total of probabilities: {round(df_prediction_probabilities['PROBABILITIES'].sum(), 2)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed7ee9-9fd7-4cf3-996c-1bddb566c681",
   "metadata": {},
   "source": [
    "We can explore some narratives that contain specific words (e.g., \"snake\"). We can see that the word \"snake\" may be related to various types of injuries such as injuries related to the use of motorized plumbing snakes, animal snake, and other. The model uses all of the words in the training data related to the injury to determine its type or class of injury/illness. Depending on how much training data the class had it may had a easier or harder time to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40663345-e947-4d8a-83b0-4e0120488673",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DF[df_DF['COMBINED_NARRATIVES'].str.contains('snake', case = False)]\\\n",
    "[['COMBINED_NARRATIVES','Injury / Illness']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f9951c-634f-43d9-9239-eb8ebc40892b",
   "metadata": {},
   "source": [
    "# NOTEBOOK END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a235530-3dc7-41ec-974d-1a057382afb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369da22c-6aa5-4281-8e97-fce47cc06037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed161e4-264a-4448-8ced-5dadd0b443a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89cf71a-4c00-4239-bb30-586adc4aca6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c85c51d-d612-4812-9c1a-b41aeb099a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb910b5-3f2f-46cc-88b9-9f45ea0595ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1448576-3789-431b-b5e9-b6602f601ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
