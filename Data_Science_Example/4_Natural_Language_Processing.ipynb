{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99e6971e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Notebook Author:<br>Felix Gonzalez, P.E. <br> Adjunct Instructor, <br> Division of Professional Studies <br> Computer Science and Electrical Engineering <br> University of Maryland Baltimore County <br> fgonzale@umbc.edu\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea27175",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Acknowledgements:<br>Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. Oâ€™Reilly Media Inc.\n",
    "<br> https://www.nltk.org/book/\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcaf154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "\n",
    "import nltk # Natural Langage Toolkit\n",
    "from nltk import word_tokenize, pos_tag # Tokenizer and Parts of Speech Tags\n",
    "from nltk.tokenize import RegexpTokenizer # Tokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer # Lemmitization and Stemming\n",
    "from nltk.corpus import stopwords, wordnet # Stopwords and POS tags\n",
    "#nltk.download #(One time to download 'stopwords')\n",
    "#nltk.download # (One time to download 'punkt')\n",
    "#nltk.download #(One time to download 'averaged_perceptron_tagger')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Vectorization Functions\n",
    "from sklearn.metrics import pairwise_distances # Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7a305c-9021-41e2-bd19-0893cf2038d5",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "There are various packages that allow you to perform natural language processing (NLP) Other NLP libraries that can be used include Gensim and SpaCy.\n",
    "Lecture 15, 31_Natural_Language_Processing_Introduction.ipynb includes more details on the capabilities of some of these packages and details on performing NLP. This notebook will focus on developing a Bag of Words (BoW) model and using the model to perform NLP tasks calculations (e.g., similarity ranking, text clustering, and classification). \n",
    "\n",
    "\n",
    "\n",
    "https://www.nltk.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef9227c",
   "metadata": {},
   "source": [
    "\n",
    "The notebook will focus on the simpler BoW model. In this notebook specifically will \n",
    "Talk about BoW\n",
    "\n",
    "Text Normalization (https://www.nltk.org/book/ch03.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c37d8f-ff24-4726-9230-c254f439860f",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0404139a-2716-4706-9bbe-6011771f6be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8afedf9d-34f4-4f08-8490-243312206613",
   "metadata": {},
   "source": [
    "# Data or Corpus Specific Stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb201df5-5cd4-40cf-a616-a1b8a9b4716f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ada80b2f-6951-4ccf-bae6-5dd3deb6e95d",
   "metadata": {},
   "source": [
    "# Text Normalization Functions\n",
    "[Return to Table of Contents](#Table-of-Contents)\n",
    "\n",
    "During text normalization various task can be performed which include but not limited to applying lower case, removing numbers and special characters, removing stop words, and applying lemmatization and/or stemming both which reduces words to their root. \n",
    "\n",
    "Documentation:\n",
    "- NLTK Library: https://www.nltk.org/\n",
    "- NLTK WordnetLemmatizer: https://www.nltk.org/_modules/nltk/stem/wordnet.html\n",
    "- NLTK Porterstemmer: https://www.nltk.org/howto/stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e8f768-df02-4a0f-ad78-e5a8208fd570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of text. \n",
    "def text_normalization(text, word_reduction_method):\n",
    "    text = str(text) # Convert narrative to string.\n",
    "    df = pd.DataFrame({'': [text]}) # Converts narrative to a dataframe format use replace functions.\n",
    "    df[''] = df[''].str.lower() # Covert narrative to lower case.\n",
    "    df[''] = df[''].str.replace(\"\\d+\", \" \", regex = True) # Remove numbers\n",
    "    df[''] = df[''].str.replace(\"[^\\w\\s]\", \" \", regex = True) # Remove special characters\n",
    "    df[''] = df[''].str.replace(\"_\", \" \", regex = True) # Remove underscores characters\n",
    "    df[''] = df[''].str.replace('\\s+', ' ', regex = True) # Replace multiple spaces with single\n",
    "    text = str(df[0:1]) # Extracts narrative from dataframe.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # Tokenizer.\n",
    "    tokens = tokenizer.tokenize(text) # Tokenize words.\n",
    "    filtered_words = [w for w in tokens if len(w) > 1 if not w in stopwords_custom] # Note remove words of 1 letter only. Can increase to higher value as needed.\n",
    "    if word_reduction_method == 'Lemmatization':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        reduced_words=[lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in filtered_words] # Lemmatization.  The second argument is the POS tag.\n",
    "    if word_reduction_method == 'Stemming':\n",
    "        stemmer = PorterStemmer() # Stemming also could make the word unreadable but is faster than lemmatization.\n",
    "        reduced_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    return \" \".join(reduced_words) # Join words with space.\n",
    "\n",
    "def get_wordnet_pos(word): # Reference: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/#wordnetlemmatizer\n",
    "    #\"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03a574b-ce2f-4035-b728-63b2ff96e47c",
   "metadata": {},
   "source": [
    "When performing NLP, you will need to decide the target feature or column. In this case we have two columns that have unstructured data. One is the question the other is the answer. We could also combine both in one column and use that as the target. In this case we will use the question as the target. This will allow us to perform various NLP tasks within the target text or category:\n",
    "- similarity calculations or ranking \n",
    "- text classification\n",
    "- text clustering \n",
    "\n",
    "However, not that the questions tend to be really short. Before making a decision on the target it would also be good practice to explore some statistics on the number of words, lenght of the text, and any other stats that we may think of. Note below that when normalizing text data it sometimes removes all the words and we end up with a target text with no words hence no vectors. This will cause issues later on and need to be addressed (i.e., fixed or removed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c0117d-453d-43c9-b8b7-2ac4ef003337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce7c3f-cedc-4b56-8a7c-c1b91f2c3958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c591e-17f4-4eec-b545-dba4a2a2e40c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bb1595-dd68-41e0-8559-90dda3d17f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6e803b-9c26-45bc-a1c5-76ea1928d8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251bb910-1e08-4e80-a7ec-886d28f4fb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a8424-8b87-411a-abee-cd922e3da6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e327bc92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a7e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
