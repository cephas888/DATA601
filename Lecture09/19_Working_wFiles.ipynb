{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e68d4f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Author:<br>Felix Gonzalez, P.E. <br> Adjunct Instructor, <br> Division of Professional Studies <br> Computer Science and Electrical Engineering <br> University of Maryland Baltimore County <br> fgonzale@umbc.edu\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4967c98d",
   "metadata": {},
   "source": [
    "This notebook provides an overview of basic concepts in workign with various types of data sources and files in the Python Programming Language and Jupyter Notebooks. These data sources include files such as txt, json, and csv. The notebook also includes a discussion of various functions and libraries as well as methods on working with multiple data sources files. As a data scientist you will have to work with data in multiple files as well as multiple data file types. In many cases you will need to consolidate, merge, or concatenate to make the dataset more useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a857c8f7",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "[Python Libraries in this Notebook](#Python-Libraries-in-this-Notebook)\n",
    "\n",
    "[OS Library](#OS-Library)\n",
    "\n",
    "- [Working Directory Path Environment](#Working-Directory-Path-Environment)\n",
    "\n",
    "- [Working in the Directory](#Working-in-the-Directory)\n",
    "\n",
    "[Pandas: Working with Multiple Files](#Pandas:-Working-with-Multiple-Files)\n",
    "\n",
    "- [Reading Multiple CSV Files and Pandas: Electricity Use Data Example](#Reading-Multiple-CSV-Files-and-Pandas:-Electricity-Use-Data-Example)\n",
    "\n",
    "[Reading Data Line By Line](#Reading-Data-Line-By-Line)\n",
    "\n",
    "[Working with PDFs](#Working-with-PDFs)\n",
    "\n",
    "[Python Built-In Functions for Loading Data (OPTIONAL)](#Python-Built-In-Functions-for-Loading-Data-(OPTIONAL))\n",
    "\n",
    "- [Open Function](#Open-Function)\n",
    "\n",
    "- [With Statement](#With-Statement)\n",
    "\n",
    "- [Example with Numerical Data in a TXT File](#Example-with-Numerical-Data-in-a-TXT-File)\n",
    "\n",
    "[JSON Library: JSON Files (OPTIONAL)](#JSON-Library:-JSON-Files-(OPTIONAL))\n",
    "\n",
    "[Working with CSV Files (CSV Library)](#Working-with-CSV-Files-(CSV-Library))\n",
    "\n",
    "[Reading Multiple TXT Files (OPTIONAL)](#Reading-Multiple-TXT-Files-(OPTIONAL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240b1ad",
   "metadata": {},
   "source": [
    "# Python Libraries in this Notebook\n",
    "[Return to Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e28cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import glob\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "import re\n",
    "import fileinput\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ee730",
   "metadata": {},
   "source": [
    "# OS Library\n",
    "[Return to Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The Operating System (OS) module provides a portable way of using OS dependent functionality. There are various useful functions and modules within the OS library. Some of these include:\n",
    "- Open() function: read or write a file  \n",
    "- Os.path module: Read and manipulate paths  \n",
    "- Fileinput module: Read all the lines in all the files on the command line\n",
    "- Tempfile module: Creating temporary files and directories \n",
    "- shutil module: High-level file and directory handling\n",
    "\n",
    "There are other libraries that also make it easy to work with files such as the Glob library.\n",
    "\n",
    "Documentation References:\n",
    "- OS Library: https://docs.python.org/3/library/os.html\n",
    "- Glob Library: https://docs.python.org/3/library/glob.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104a01a",
   "metadata": {},
   "source": [
    "There are a few things to remember and that will vary from system to system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "956d5e87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.linesep # Note on line separator which will vary depending in the OS system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f93dc3b",
   "metadata": {},
   "source": [
    "From the OS Library documentation:\n",
    "\n",
    "os.linesep: \"The string used to separate (or, rather, terminate) lines on the current platform. This may be a single character, such as '\\n' for POSIX, or multiple characters, for example, '\\r\\n' for Windows. Do not use os.linesep as a line terminator when writing files opened in text mode (the default); use a single '\\n' instead, on all platforms.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65651b5",
   "metadata": {},
   "source": [
    "Also recall that paths in windows uses the \"\\\" and in Unix/MacOS is \"/\" and depending on your operating system you may get different resutls in the cell above. The Unix style path works on both operating systems whithin the Python environment and should be the __preferred__.  Recall that the \\ is also an escape character and __NOT__ the preferred approach.\n",
    "\n",
    "References:\n",
    "- https://stackoverflow.com/questions/1589930/so-what-is-the-right-direction-of-the-paths-slash-or-under-windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1701171b",
   "metadata": {},
   "source": [
    "#### Working Directory Path Environment\n",
    "[Return to Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adb98132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\anaconda3;C:\\Users\\felix\\anaconda3\\Library\\mingw-w64\\bin;C:\\Users\\felix\\anaconda3\\Library\\usr\\bin;C:\\Users\\felix\\anaconda3\\Library\\bin;C:\\Users\\felix\\anaconda3\\Scripts;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Users\\felix\\AppData\\Local\\Microsoft\\WindowsApps;\n"
     ]
    }
   ],
   "source": [
    "print(os.environ[\"PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1d876c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'HOME'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# If in Windows, the home environment/folder will not work. See next cell.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHOME\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m<frozen os>:679\u001b[0m, in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'HOME'"
     ]
    }
   ],
   "source": [
    "# If in Windows, the home environment/folder will not work. See next cell.\n",
    "print(os.environ[\"HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3476025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\n"
     ]
    }
   ],
   "source": [
    "#If in Windows need to use the follwoing instead of os.environ[\"HOME\"]\n",
    "home_folder = os.path.expanduser('~')\n",
    "print(home_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b31c2c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Felix_ASUS_Docs\\\\1A_Python_Projects\\\\DATA601_Files\\\\Lecture09'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Current working directory.\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd92d6",
   "metadata": {},
   "source": [
    "#### Working in the Directory\n",
    "[Return to Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0a988b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '19_Working_wFiles.ipynb',\n",
       " '20_Working_with_WebData_and_APIs.ipynb',\n",
       " '21a_SQL_DB_Test_File_Creation.ipynb',\n",
       " '21b_Relational_Databases_and_SQL.ipynb',\n",
       " 'database.sqlite',\n",
       " 'input_data',\n",
       " 'output_data',\n",
       " 'test_file.txt',\n",
       " 'test_folder']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of everything in the present directory (Note: Does not include files in subfolders)\n",
    "os.listdir(path='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44fe4126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the command mkdir (make a directory) in the system shell\n",
    "os.system('mkdir test_folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca802709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '19_Working_wFiles.ipynb',\n",
       " '20_Working_with_WebData_and_APIs.ipynb',\n",
       " '21a_SQL_DB_Test_File_Creation.ipynb',\n",
       " '21b_Relational_Databases_and_SQL.ipynb',\n",
       " 'database.sqlite',\n",
       " 'input_data',\n",
       " 'output_data',\n",
       " 'test_file.txt',\n",
       " 'test_folder']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list everything in the present directory\n",
    "os.listdir(path='.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c4533b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have a new folder called \"test_folder\". let's go into that directory\n",
    "os.chdir('./test_folder/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b73c4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Felix_ASUS_Docs\\\\1A_Python_Projects\\\\DATA601_Files\\\\Lecture09\\\\test_folder'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5e5df3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Felix_ASUS_Docs\\\\1A_Python_Projects\\\\DATA601_Files\\\\Lecture09'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's go back to previous folder\n",
    "os.chdir('./..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2e46109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('mkdir test_folder2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95a77744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a folder inside the test_folder2\n",
    "#os.system('mkdir test_folder2/test_subfolder1')\n",
    "\n",
    "# Alternatively to os.system, we can use the os.mkdir.\n",
    "os.mkdir('test_folder2/test_subfolder1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28dee1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints\n",
      "input_data\n",
      "output_data\n",
      "test_folder\n",
      "test_folder2\n"
     ]
    }
   ],
   "source": [
    "# let's list subdirectories using the pathlib library\n",
    "p = pathlib.Path('.') # current directory\n",
    "for x in p.iterdir():\n",
    "    if x.is_dir():\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fa01b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING A FILE AT CURRENT DIRECTORY\n",
    "f = open('test_file.txt',\"w+\")\n",
    "\n",
    "# WRITING IN A FILE\n",
    "for x in range(10):\n",
    "    f.write(\"This is line %d\\r\\n\" % (x+1))\n",
    "    \n",
    "# CLOSING A FILE\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "520595b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENING AN EXISTING FILE AND APPENDING IT\n",
    "g = open(\"test_file.txt\",\"+a\")\n",
    "\n",
    "for x in range(5):\n",
    "    g.write(\"Appended line %d\\r\\n\" % (x+1))\n",
    "\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f94fcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is line 1\n",
      "\n",
      "This is line 2\n",
      "\n",
      "This is line 3\n",
      "\n",
      "This is line 4\n",
      "\n",
      "This is line 5\n",
      "\n",
      "This is line 6\n",
      "\n",
      "This is line 7\n",
      "\n",
      "This is line 8\n",
      "\n",
      "This is line 9\n",
      "\n",
      "This is line 10\n",
      "\n",
      "Appended line 1\n",
      "\n",
      "Appended line 2\n",
      "\n",
      "Appended line 3\n",
      "\n",
      "Appended line 4\n",
      "\n",
      "Appended line 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# READING A LOCAL FILE LINE BY LINE (More on this in later sections).\n",
    "# See section \"Reading Data Line By Line\"\n",
    "with open('test_file.txt','r') as h:\n",
    "    for line in h:\n",
    "        print(line.rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04c7fb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints\n",
      "input_data\n",
      "output_data\n",
      "test_folder\n",
      "test_folder2\n"
     ]
    }
   ],
   "source": [
    "# let's list subdirectories (if any) using the pathlib library\n",
    "p = pathlib.Path('.') # current directory\n",
    "for x in p.iterdir():\n",
    "    if x.is_dir():\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c767cdf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 145] The directory is not empty: 'test_folder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Let's remove the two directories that we created.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m os\u001b[38;5;241m.\u001b[39mrmdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_folder\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mrmdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_folder2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 145] The directory is not empty: 'test_folder'"
     ]
    }
   ],
   "source": [
    "# Let's remove the two directories that we created.\n",
    "os.rmdir('test_folder')\n",
    "os.rmdir('test_folder2') # Will give an error because the folder is not empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "032b9d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove directory and subdirectories use the shutil library\n",
    "shutil.rmtree('./test_folder2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bac7e1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints\n",
      "input_data\n",
      "output_data\n",
      "test_folder\n"
     ]
    }
   ],
   "source": [
    "# Let's list subdirectories (if any) using the pathlib library\n",
    "p = pathlib.Path('.') # current directory\n",
    "for x in p.iterdir():\n",
    "    if x.is_dir():\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a2956b",
   "metadata": {},
   "source": [
    "Using the glob library let's list all the txt files in this folder and subfolders using the glob library. The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order. \n",
    "\n",
    "Documentation References:\n",
    "- https://docs.python.org/3/library/glob.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba13f580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_file.txt',\n",
       " 'input_data\\\\anotherfile.txt',\n",
       " 'input_data\\\\bread.txt',\n",
       " 'input_data\\\\ex1data1.txt',\n",
       " 'input_data\\\\text_files\\\\example3.txt',\n",
       " 'input_data\\\\text_files\\\\FAME.TXT',\n",
       " 'input_data\\\\text_files\\\\Genescan.txt',\n",
       " 'output_data\\\\anotherfile.txt',\n",
       " 'output_data\\\\json_data.txt',\n",
       " 'output_data\\\\textout.txt',\n",
       " 'test_folder\\\\test_file.txt']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob('**/*.txt', recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb9b12",
   "metadata": {},
   "source": [
    "Another option to obtain all the files in the directory is to use the os.walk() function.\n",
    "\n",
    "Documentation References:\n",
    "- os.walk(): https://docs.python.org/3/library/os.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9730caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_tuple = [x for x in os.walk(top = './')] # Produces a 3-tuple of (dirpath, dirnames, filenames)\n",
    "\n",
    "all_files_dir = [] # Defines a starting empty list for the all_files_dir.\n",
    "\n",
    "for i in range(len(all_files_tuple)): # Iterates thru each index or element in all_files_tuple.\n",
    "    # Iterates thru files in dir and only select files in directory with specified extension (case insensitive).\n",
    "    txt_files = [filename for filename in os.listdir(all_files_tuple[i][0]) if filename.lower().endswith('.txt'.lower())]\n",
    "    # Combines both the relative path and filename\n",
    "    txt_files = [all_files_tuple[i][0] +'/'+ filename  for filename in txt_files]\n",
    "    # Consolidates all files from the path being iterated into the main list.\n",
    "    all_files_dir = all_files_dir + txt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cf2b169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('./',\n",
       "  ['.ipynb_checkpoints', 'input_data', 'output_data', 'test_folder'],\n",
       "  ['19_Working_wFiles.ipynb',\n",
       "   '20_Working_with_WebData_and_APIs.ipynb',\n",
       "   '21a_SQL_DB_Test_File_Creation.ipynb',\n",
       "   '21b_Relational_Databases_and_SQL.ipynb',\n",
       "   'database.sqlite',\n",
       "   'test_file.txt']),\n",
       " ('./.ipynb_checkpoints',\n",
       "  [],\n",
       "  ['19_Working_wFiles-checkpoint.ipynb',\n",
       "   '20_Working_with_WebData_and_APIs-checkpoint.ipynb',\n",
       "   '20_Working_with_WebData_and_APIs-Copy1-checkpoint.ipynb',\n",
       "   '21a_SQL_DB_Test_File_Creation-checkpoint.ipynb',\n",
       "   '21b_Relational_Databases_and_SQL-checkpoint.ipynb',\n",
       "   '21_Pandas_Relational_Data_Overview-checkpoint.ipynb']),\n",
       " ('./input_data',\n",
       "  ['airline_data', 'electricity_use_data_cleaned', 'text_files'],\n",
       "  ['addresses.csv',\n",
       "   'anotherfile.txt',\n",
       "   'bread.txt',\n",
       "   'csv_plain_file.csv',\n",
       "   'ex1data1.txt',\n",
       "   'geo_data.json',\n",
       "   'NRC_ASP_DATA_from_Public_ASP_Dashboard.csv']),\n",
       " ('./input_data\\\\airline_data',\n",
       "  [],\n",
       "  ['airlines.csv', 'airports.csv', 'flights.csv']),\n",
       " ('./input_data\\\\electricity_use_data_cleaned',\n",
       "  [],\n",
       "  ['20737_1006_electric_singlefamily_primary_cleaned.csv',\n",
       "   '20871_2400_GAS_TOWNHOUSE_PRIMARY_cleaned.csv',\n",
       "   '20871_3200_electric_singlefamily_primary_cleaned.csv',\n",
       "   '20874_NA_ELECTRIC_Apartiment_PRIMARY_cleaned.csv',\n",
       "   '20904_1700_ELECTRIC_TOWNHOUSE_PRIMARY_cleaned.csv',\n",
       "   'ZIPCODE_SQFEET_HEATERTYPE_HOUSETYPE_HOUSEUSAGE-TEMPLATE.csv']),\n",
       " ('./input_data\\\\text_files',\n",
       "  [],\n",
       "  ['example3.txt', 'FAME.TXT', 'Genescan.txt']),\n",
       " ('./output_data',\n",
       "  ['airline_data', 'API_initial_download'],\n",
       "  ['anotherfile.txt', 'dataout1.csv', 'json_data.txt', 'textout.txt']),\n",
       " ('./output_data\\\\airline_data',\n",
       "  [],\n",
       "  ['df_flights_cleaned_2.csv', 'flights_database.db']),\n",
       " ('./output_data\\\\API_initial_download',\n",
       "  [],\n",
       "  ['fed_register_data_accident.json', 'fed_register_data_weather.json']),\n",
       " ('./test_folder', [], ['test_file.txt'])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e87ea50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.//test_file.txt',\n",
       " './input_data/anotherfile.txt',\n",
       " './input_data/bread.txt',\n",
       " './input_data/ex1data1.txt',\n",
       " './input_data\\\\text_files/example3.txt',\n",
       " './input_data\\\\text_files/FAME.TXT',\n",
       " './input_data\\\\text_files/Genescan.txt',\n",
       " './output_data/anotherfile.txt',\n",
       " './output_data/json_data.txt',\n",
       " './output_data/textout.txt',\n",
       " './test_folder/test_file.txt']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files_dir # The output here is similar to the output in the glob.glob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0e436e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find files that are in subfolders only.\n",
    "sub_folders = [] # Defines a starting empty List of subfolder names that will have specified file type (e.g., csv).\n",
    "\n",
    "# Iterates thru all file directories to extract Folder name. Located at all_files_dir[i].split('/')[2]\n",
    "for i in range(len(all_files_dir)):\n",
    "    sub_folders = sub_folders + [all_files_dir[i].split('/')[2]]\n",
    "sub_folders = list(set(sub_folders)) # Creates unique list of files in subfolders by removing duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21bb57f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bread.txt',\n",
       " 'ex1data1.txt',\n",
       " 'anotherfile.txt',\n",
       " 'FAME.TXT',\n",
       " 'example3.txt',\n",
       " 'json_data.txt',\n",
       " 'textout.txt',\n",
       " 'test_file.txt',\n",
       " 'Genescan.txt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_folders # List of files in subfolders (No duplicates) Can remove the set if there would be duplicate names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3de96e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_data\\\\anotherfile.txt',\n",
       " 'input_data\\\\bread.txt',\n",
       " 'input_data\\\\ex1data1.txt',\n",
       " 'input_data\\\\text_files\\\\example3.txt',\n",
       " 'input_data\\\\text_files\\\\FAME.TXT',\n",
       " 'input_data\\\\text_files\\\\Genescan.txt',\n",
       " 'output_data\\\\anotherfile.txt',\n",
       " 'output_data\\\\json_data.txt',\n",
       " 'output_data\\\\textout.txt',\n",
       " 'test_folder\\\\test_file.txt']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's remove the test_file.txt that we created earlier and check the txt files in the directory.\n",
    "os.remove('./test_file.txt')\n",
    "glob.glob('**/*.txt', recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f40d870",
   "metadata": {},
   "source": [
    "# Pandas: Working with Multiple Files\n",
    "[Return to Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Up to this moment we have used Pandas to load data from a single file and in a few cases join/merge/concatenate with data from another file. As a data scientist you will have to work with data in multiple files and in many cases you will need to consolidate, merge, or concatenate to make the dataset more useful. This section discusses some examples on working with multiple files using OS and Pandas library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc3839b",
   "metadata": {},
   "source": [
    "#### Reading Multiple CSV Files and Pandas: Electricity Use Data Example\n",
    "[Return to Table of Contents](#Table-of-Contents)\n",
    "\n",
    "When reading and extracting data from multiple files there may be various ways. In this case we will explore extracting data from the electricity use data that we collected at the beginning of the class and explore various challenges that we may encounter when processing the data files. More often than not, when the data was collected manually by various people there may be differences in the files that may provide challenges on how the data is extracted.\n",
    "\n",
    "The first step is to manually get familiarized with the datasets as well as the columns and features collected. Once the feature names are explored and normalized (i.e., made the same) we can continue to merge the datasets and extract any data that we may need to extract from the filename or the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49bc1d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('./input_data/electricity_use_data_cleaned/',\n",
       "  [],\n",
       "  ['20737_1006_electric_singlefamily_primary_cleaned.csv',\n",
       "   '20871_2400_GAS_TOWNHOUSE_PRIMARY_cleaned.csv',\n",
       "   '20871_3200_electric_singlefamily_primary_cleaned.csv',\n",
       "   '20874_NA_ELECTRIC_Apartiment_PRIMARY_cleaned.csv',\n",
       "   '20904_1700_ELECTRIC_TOWNHOUSE_PRIMARY_cleaned.csv',\n",
       "   'ZIPCODE_SQFEET_HEATERTYPE_HOUSETYPE_HOUSEUSAGE-TEMPLATE.csv'])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See Documentation for os.walk(): https://docs.python.org/3/library/os.html\n",
    "# Produces a 3-tuple of (dirpath, dirnames, filenames)\n",
    "all_files_tuple = [x for x in os.walk(top = './input_data/electricity_use_data_cleaned/')] # Produces a 3-tuple of (dirpath, dirnames, filenames)\n",
    "all_files_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdbb5668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20737_1006_electric_singlefamily_primary_cleaned.csv',\n",
       " '20871_2400_GAS_TOWNHOUSE_PRIMARY_cleaned.csv',\n",
       " '20871_3200_electric_singlefamily_primary_cleaned.csv',\n",
       " '20874_NA_ELECTRIC_Apartiment_PRIMARY_cleaned.csv',\n",
       " '20904_1700_ELECTRIC_TOWNHOUSE_PRIMARY_cleaned.csv',\n",
       " 'ZIPCODE_SQFEET_HEATERTYPE_HOUSETYPE_HOUSEUSAGE-TEMPLATE.csv']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the list of the file names.\n",
    "all_files_tuple[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a14a289f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List has no duplicate files: True.\n",
      "There are 6 specified file type (e.g., csv).\n",
      "Unique subfolders with specified file type (e.g., csv): {'electricity_use_data_cleaned'}.\n",
      "Sample of three files: ['./input_data/electricity_use_data_cleaned//20737_1006_electric_singlefamily_primary_cleaned.csv', './input_data/electricity_use_data_cleaned//20871_2400_GAS_TOWNHOUSE_PRIMARY_cleaned.csv', './input_data/electricity_use_data_cleaned//20871_3200_electric_singlefamily_primary_cleaned.csv'].\n"
     ]
    }
   ],
   "source": [
    "# Goal: I want to obtain a list of the specified type file (e.g., csv) with their relative directory to later iterate thru.\n",
    "all_files_dir = [] # Defines a starting empty list for the all_files_dir.\n",
    "sub_folders = [] # Defines a starting empty List of subfolder names that will have specified file type (e.g., csv).\n",
    "\n",
    "for i in range(len(all_files_tuple)): # Iterates thru each index or element in all_files_tuple.\n",
    "    # Iterates thru files in dir and only select files in directory with specified extension (case insensitive).\n",
    "    csv_files = [filename for filename in os.listdir(all_files_tuple[i][0]) if filename.lower().endswith('.csv'.lower())]\n",
    "    # Combines both the relative path and filename\n",
    "    csv_files = [all_files_tuple[i][0] +'/'+ filename  for filename in csv_files]\n",
    "    # Consolidates all files from the path being iterated into the main list.\n",
    "    all_files_dir = all_files_dir + csv_files\n",
    "    \n",
    "# Iterates thru all file directories to extract Folder name. Located at all_files_dir[i].split('/')[2]\n",
    "#for i in range(len(all_files_dir)):\n",
    "    sub_folders = sub_folders + [all_files_dir[i].split('/')[2]]   \n",
    "sub_folders = set(sub_folders) # Creates unique list of subfolders by removing duplicates.\n",
    "# The subfolders could mean a year a location a zipcode or some other important information.\n",
    "# Because there are no subfolders in the input_data/electricity_use_data_cleaned this will be an empty list.\n",
    "\n",
    "# Checks if final list has not duplicate files.\n",
    "print(f'List has no duplicate files: {len(all_files_dir) == len(set(all_files_dir))}.')\n",
    "print(f'There are {len(all_files_dir)} specified file type (e.g., csv).') # Shows how many files are in the final list.\n",
    "print(f'Unique subfolders with specified file type (e.g., csv): {sub_folders}.') # Show list of sub-folders.\n",
    "print(f'Sample of three files: {all_files_dir[:3]}.') # Shows only the first 3 files in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45cd45d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./input_data/electricity_use_data_cleaned//20737_1006_electric_singlefamily_primary_cleaned.csv',\n",
       " './input_data/electricity_use_data_cleaned//20871_2400_GAS_TOWNHOUSE_PRIMARY_cleaned.csv',\n",
       " './input_data/electricity_use_data_cleaned//20871_3200_electric_singlefamily_primary_cleaned.csv',\n",
       " './input_data/electricity_use_data_cleaned//20874_NA_ELECTRIC_Apartiment_PRIMARY_cleaned.csv',\n",
       " './input_data/electricity_use_data_cleaned//20904_1700_ELECTRIC_TOWNHOUSE_PRIMARY_cleaned.csv',\n",
       " './input_data/electricity_use_data_cleaned//ZIPCODE_SQFEET_HEATERTYPE_HOUSETYPE_HOUSEUSAGE-TEMPLATE.csv']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files_dir # In this case there are only a handful of files so we can see the full list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaab2cc",
   "metadata": {},
   "source": [
    "After we know the file paths for the data that we want, we have various options:\n",
    "1. We can initialize the main dataframe by creating a new blank dataframe with the required columns \n",
    "2. Use the columns from the TEMPLATE but we also need to add the columns related to the file names.\n",
    "\n",
    "However, before attempting this we should manually open a few of the CSV files to see if we can spot potential challenges or problems. The original files submitted by students can be found under 'Homework/Special_HW_Home_Electricity_Consumption' folder. The files in the input_data/electricity_use_data_cleaned has been manually fixed. Issues that we can observe in the data:\n",
    "- Different date formats\n",
    "- Some dataframes used a date range while others estimated the month and year. \n",
    "- Some others added extra data that seemed to be available.\n",
    "- Issues with column naming.\n",
    "\n",
    "Note that the instructions were vague on purpose to demonstrate potential issues that may arise during the data collection stage. Even when instructions are clear you will encounter challenges like this. Coordinating during data collection can make improvements in the quality of the data and improve the insights when it is analyzed. Issues in the data collection can sometimes cause that the data is unusable and having to be discarded. \n",
    "\n",
    "There are a few things that we can do to solve the issue.\n",
    "1. If we only had a handful of files we can manually make the change (which is the case here). In some cases we may have to do some assumptions to make the data usable.\n",
    "2. If we had lots of files (e.g., hundreds) we may need to create an if statement that detects the type of file issue, then do either of:\n",
    "    a. fix the issue before extracting\n",
    "    b. extract the data from the appropriate location\n",
    "\n",
    "Another issue that we could probably see is that given the current structure of the dataset, there will be a high likelihood that we may have duplicates in the data. For example, if we obtain more data, there is a high likelihood that our neighbors data may be very similar and in case of the filenaming exactly the same. This will cause issues as the operating systems do not allow files to have the same filename. The address could have been a good addition to the filename to avoid this. \n",
    "\n",
    "Depending on the issue addressing them may be easy, to very difficult to impossible. Option 2 above assumes that the issues is consistent within various files and that the majority of the data collected was using the correct template or format and hence the issue can use an if statement to be identified.. Imagine having this issues accross hundreds of files that we need to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd9146e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./input_data/electricity_use_data_cleaned//ZIPCODE_SQFEET_HEATERTYPE_HOUSETYPE_HOUSEUSAGE-TEMPLATE.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Date</td>\n",
       "      <td>Bill Amount</td>\n",
       "      <td>Days in Billing Cycle</td>\n",
       "      <td>KWH Usage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0            1                      2          3\n",
       "0  Date  Bill Amount  Days in Billing Cycle  KWH Usage\n",
       "1   NaN          NaN                    NaN        NaN\n",
       "2   NaN          NaN                    NaN        NaN\n",
       "3   NaN          NaN                    NaN        NaN\n",
       "4   NaN          NaN                    NaN        NaN"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's explore the TEMPLATE dataframe, other files and explore their columns.\n",
    "# Note that the columns are in the first row.\n",
    "i = -1 # Use from -1 to zero to postive.\n",
    "print(csv_files[i])\n",
    "pd.read_csv(csv_files[i], header = None).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa411f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>Home Square Footage</th>\n",
       "      <th>Heater Type</th>\n",
       "      <th>Home Type</th>\n",
       "      <th>Home Usage</th>\n",
       "      <th>Date</th>\n",
       "      <th>Bill Amount</th>\n",
       "      <th>Days in Billing Cycle</th>\n",
       "      <th>KWH Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Zipcode, Home Square Footage, Heater Type, Home Type, Home Usage, Date, Bill Amount, Days in Billing Cycle, KWH Usage]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to combine all the dataframes with electricity use data including the data in the filename\n",
    "# One approache could be to initialize a main empty dataframe where the data gets added with the right columns.\n",
    "# Let's initialize an empty main DataFrame with the required columns.\n",
    "df_electricity_usage = pd.DataFrame(columns = ['Zipcode', 'Home Square Footage', 'Heater Type', 'Home Type', \n",
    "                                               'Home Usage', 'Date', 'Bill Amount', 'Days in Billing Cycle', \n",
    "                                               'KWH Usage'])\n",
    "# Note that some of the data will be in the filename while other data will be inside the file.\n",
    "df_electricity_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2f39df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20737', '1006', 'electric', 'singlefamily', 'primary', 'cleaned.csv']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's explore the data for file at index 0.\n",
    "i = 0\n",
    "file_name_list = csv_files[i].split(\"/\")[4].split('_')\n",
    "file_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19e49083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home Usage</th>\n",
       "      <th>Date</th>\n",
       "      <th>Bill Amount</th>\n",
       "      <th>Days in Billing Cycle</th>\n",
       "      <th>KWH Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>primary</td>\n",
       "      <td>2021 December</td>\n",
       "      <td>$117.03</td>\n",
       "      <td>32</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>primary</td>\n",
       "      <td>2022 January</td>\n",
       "      <td>$156.02</td>\n",
       "      <td>27</td>\n",
       "      <td>1114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>primary</td>\n",
       "      <td>2022 February</td>\n",
       "      <td>$98.69</td>\n",
       "      <td>28</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>primary</td>\n",
       "      <td>2022 March</td>\n",
       "      <td>$78.70</td>\n",
       "      <td>30</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>primary</td>\n",
       "      <td>2022 April</td>\n",
       "      <td>$67.62</td>\n",
       "      <td>30</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>primary</td>\n",
       "      <td>2022 May</td>\n",
       "      <td>$144.69</td>\n",
       "      <td>29</td>\n",
       "      <td>851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>primary</td>\n",
       "      <td>2022 June</td>\n",
       "      <td>$226.59</td>\n",
       "      <td>32</td>\n",
       "      <td>1436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>primary</td>\n",
       "      <td>2022 July</td>\n",
       "      <td>$221.60</td>\n",
       "      <td>27</td>\n",
       "      <td>1445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>primary</td>\n",
       "      <td>2022 August</td>\n",
       "      <td>$235.64</td>\n",
       "      <td>31</td>\n",
       "      <td>1436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>primary</td>\n",
       "      <td>2022 September</td>\n",
       "      <td>$145.08</td>\n",
       "      <td>30</td>\n",
       "      <td>884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>primary</td>\n",
       "      <td>2022 October</td>\n",
       "      <td>$93.93</td>\n",
       "      <td>28</td>\n",
       "      <td>566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>primary</td>\n",
       "      <td>2022 November</td>\n",
       "      <td>$168.16</td>\n",
       "      <td>29</td>\n",
       "      <td>1057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Home Usage            Date Bill Amount  Days in Billing Cycle  KWH Usage\n",
       "0     primary   2021 December    $117.03                      32        857\n",
       "1     primary    2022 January    $156.02                      27       1114\n",
       "2     primary   2022 February     $98.69                      28        681\n",
       "3     primary      2022 March     $78.70                      30        532\n",
       "4     primary      2022 April     $67.62                      30        429\n",
       "5     primary        2022 May    $144.69                      29        851\n",
       "6     primary       2022 June    $226.59                      32       1436\n",
       "7     primary       2022 July    $221.60                      27       1445\n",
       "8     primary     2022 August    $235.64                      31       1436\n",
       "9     primary  2022 September    $145.08                      30        884\n",
       "10    primary    2022 October     $93.93                      28        566\n",
       "11    primary   2022 November    $168.16                      29       1057"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf = pd.read_csv(csv_files[i], header = 0)\n",
    "# We can add a column of the filename information with the insert function.\n",
    "testdf.insert(0, \"Home Usage\", file_name_list[4])\n",
    "testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90b517e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create a loop that iterates thru all the files.\n",
    "for i in range(len(csv_files)):\n",
    "    # Loading csv file as temporary df.\n",
    "    tempdf = pd.read_csv(csv_files[i], header = 0) # Header located at row index 0.\n",
    "    # Filename information in a list using underscore as delimiter.\n",
    "    file_name_list = csv_files[i].split(\"/\")[4].split('_')\n",
    "    # Inserts the new columsn for Zipcode, Home Square Footage, Heater type, and Home Usage\n",
    "    tempdf.insert(0, \"Home Usage\", file_name_list[4])\n",
    "    tempdf.insert(0, \"Home Type\", file_name_list[3])\n",
    "    tempdf.insert(0, \"Heater Type\", file_name_list[2])\n",
    "    tempdf.insert(0, \"Home Square Footage\", file_name_list[1])\n",
    "    tempdf.insert(0, \"Zipcode\", file_name_list[0])\n",
    "\n",
    "    # Before concatenating, \n",
    "    # Let's check the tempdf has the same number of features as the main dataframe (e.g., df_electricity_usage).\n",
    "    try:\n",
    "        if all(df_electricity_usage.columns == tempdf.columns): # If columns don't match will give an error hnce the try-except.\n",
    "            df_electricity_usage = pd.concat([df_electricity_usage, tempdf], axis=0) # Adds tempdf at bottom of main df.\n",
    "    except:\n",
    "        print(f'PROCESS STOPPED: Following file does not has the same features/columns:')\n",
    "        print(f'{csv_files[i]}') # Prints the file with the issue.\n",
    "        break # If there is a row that does not meet this condition for loop breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98202bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>Home Square Footage</th>\n",
       "      <th>Heater Type</th>\n",
       "      <th>Home Type</th>\n",
       "      <th>Home Usage</th>\n",
       "      <th>Date</th>\n",
       "      <th>Bill Amount</th>\n",
       "      <th>Days in Billing Cycle</th>\n",
       "      <th>KWH Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20737</td>\n",
       "      <td>1006</td>\n",
       "      <td>electric</td>\n",
       "      <td>singlefamily</td>\n",
       "      <td>primary</td>\n",
       "      <td>2021 December</td>\n",
       "      <td>$117.03</td>\n",
       "      <td>32</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20737</td>\n",
       "      <td>1006</td>\n",
       "      <td>electric</td>\n",
       "      <td>singlefamily</td>\n",
       "      <td>primary</td>\n",
       "      <td>2022 January</td>\n",
       "      <td>$156.02</td>\n",
       "      <td>27</td>\n",
       "      <td>1114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20737</td>\n",
       "      <td>1006</td>\n",
       "      <td>electric</td>\n",
       "      <td>singlefamily</td>\n",
       "      <td>primary</td>\n",
       "      <td>2022 February</td>\n",
       "      <td>$98.69</td>\n",
       "      <td>28</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20737</td>\n",
       "      <td>1006</td>\n",
       "      <td>electric</td>\n",
       "      <td>singlefamily</td>\n",
       "      <td>primary</td>\n",
       "      <td>2022 March</td>\n",
       "      <td>$78.70</td>\n",
       "      <td>30</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20737</td>\n",
       "      <td>1006</td>\n",
       "      <td>electric</td>\n",
       "      <td>singlefamily</td>\n",
       "      <td>primary</td>\n",
       "      <td>2022 April</td>\n",
       "      <td>$67.62</td>\n",
       "      <td>30</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ZIPCODE</td>\n",
       "      <td>SQFEET</td>\n",
       "      <td>HEATERTYPE</td>\n",
       "      <td>HOUSETYPE</td>\n",
       "      <td>HOUSEUSAGE-TEMPLATE.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ZIPCODE</td>\n",
       "      <td>SQFEET</td>\n",
       "      <td>HEATERTYPE</td>\n",
       "      <td>HOUSETYPE</td>\n",
       "      <td>HOUSEUSAGE-TEMPLATE.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ZIPCODE</td>\n",
       "      <td>SQFEET</td>\n",
       "      <td>HEATERTYPE</td>\n",
       "      <td>HOUSETYPE</td>\n",
       "      <td>HOUSEUSAGE-TEMPLATE.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ZIPCODE</td>\n",
       "      <td>SQFEET</td>\n",
       "      <td>HEATERTYPE</td>\n",
       "      <td>HOUSETYPE</td>\n",
       "      <td>HOUSEUSAGE-TEMPLATE.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ZIPCODE</td>\n",
       "      <td>SQFEET</td>\n",
       "      <td>HEATERTYPE</td>\n",
       "      <td>HOUSETYPE</td>\n",
       "      <td>HOUSEUSAGE-TEMPLATE.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Zipcode Home Square Footage Heater Type     Home Type  \\\n",
       "0     20737                1006    electric  singlefamily   \n",
       "1     20737                1006    electric  singlefamily   \n",
       "2     20737                1006    electric  singlefamily   \n",
       "3     20737                1006    electric  singlefamily   \n",
       "4     20737                1006    electric  singlefamily   \n",
       "..      ...                 ...         ...           ...   \n",
       "8   ZIPCODE              SQFEET  HEATERTYPE     HOUSETYPE   \n",
       "9   ZIPCODE              SQFEET  HEATERTYPE     HOUSETYPE   \n",
       "10  ZIPCODE              SQFEET  HEATERTYPE     HOUSETYPE   \n",
       "11  ZIPCODE              SQFEET  HEATERTYPE     HOUSETYPE   \n",
       "12  ZIPCODE              SQFEET  HEATERTYPE     HOUSETYPE   \n",
       "\n",
       "                 Home Usage           Date Bill Amount Days in Billing Cycle  \\\n",
       "0                   primary  2021 December    $117.03                     32   \n",
       "1                   primary   2022 January    $156.02                     27   \n",
       "2                   primary  2022 February     $98.69                     28   \n",
       "3                   primary     2022 March     $78.70                     30   \n",
       "4                   primary     2022 April     $67.62                     30   \n",
       "..                      ...            ...         ...                   ...   \n",
       "8   HOUSEUSAGE-TEMPLATE.csv            NaN         NaN                   NaN   \n",
       "9   HOUSEUSAGE-TEMPLATE.csv            NaN         NaN                   NaN   \n",
       "10  HOUSEUSAGE-TEMPLATE.csv            NaN         NaN                   NaN   \n",
       "11  HOUSEUSAGE-TEMPLATE.csv            NaN         NaN                   NaN   \n",
       "12  HOUSEUSAGE-TEMPLATE.csv            NaN         NaN                   NaN   \n",
       "\n",
       "   KWH Usage  \n",
       "0        857  \n",
       "1       1114  \n",
       "2        681  \n",
       "3        532  \n",
       "4        429  \n",
       "..       ...  \n",
       "8        NaN  \n",
       "9        NaN  \n",
       "10       NaN  \n",
       "11       NaN  \n",
       "12       NaN  \n",
       "\n",
       "[91 rows x 9 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_electricity_usage.shape)\n",
    "df_electricity_usage#.head(60)\n",
    "# Note there are 91 rows but the last row index is 12?\n",
    "# Should have used or should use the reset_index.\n",
    "# If this goes unnoticed you will potentially have issues later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "819fe04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1006'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What happens if I use an index based data selection and filter?\n",
    "df_electricity_usage.iloc[0, 1] # Only seems to select the first index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "717a383a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>Home Square Footage</th>\n",
       "      <th>Heater Type</th>\n",
       "      <th>Home Type</th>\n",
       "      <th>Home Usage</th>\n",
       "      <th>Date</th>\n",
       "      <th>Bill Amount</th>\n",
       "      <th>Days in Billing Cycle</th>\n",
       "      <th>KWH Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20737</td>\n",
       "      <td>1006</td>\n",
       "      <td>electric</td>\n",
       "      <td>singlefamily</td>\n",
       "      <td>primary</td>\n",
       "      <td>2021 December</td>\n",
       "      <td>$117.03</td>\n",
       "      <td>32</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20737</td>\n",
       "      <td>1006</td>\n",
       "      <td>electric</td>\n",
       "      <td>singlefamily</td>\n",
       "      <td>primary</td>\n",
       "      <td>2022 January</td>\n",
       "      <td>$156.02</td>\n",
       "      <td>27</td>\n",
       "      <td>1114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20737</td>\n",
       "      <td>1006</td>\n",
       "      <td>electric</td>\n",
       "      <td>singlefamily</td>\n",
       "      <td>primary</td>\n",
       "      <td>2022 February</td>\n",
       "      <td>$98.69</td>\n",
       "      <td>28</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Zipcode Home Square Footage Heater Type     Home Type Home Usage  \\\n",
       "0   20737                1006    electric  singlefamily    primary   \n",
       "1   20737                1006    electric  singlefamily    primary   \n",
       "2   20737                1006    electric  singlefamily    primary   \n",
       "\n",
       "            Date Bill Amount Days in Billing Cycle KWH Usage  \n",
       "0  2021 December    $117.03                     32       857  \n",
       "1   2022 January    $156.02                     27      1114  \n",
       "2  2022 February     $98.69                     28       681  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_electricity_usage.iloc[0:3, :] # Only seems to select the first index range 0 to 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b5afce",
   "metadata": {},
   "source": [
    "A few observations:\n",
    "- Depending on the approach you used and your dataset it may be a good idea to check for duplicated values. If there are duplicated values check the source and if they are true duplicates.\n",
    "- In the case above the repeated rows seem to be from the template. \n",
    "- Reset index is an invaluable tool especially when creating dataframes or copying a subset of a dataframe. There may be reasons to keep the original index dataframe (i.e., when you want to reference back. \n",
    "\n",
    "Once the data is combined we can use various of the functions we have learned to explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0a7f725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Home Square Footage\n",
       "2400      27\n",
       "3200      13\n",
       "NA        13\n",
       "1700      13\n",
       "SQFEET    13\n",
       "1006      12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_electricity_usage['Home Square Footage'].value_counts()\n",
    "# Here we can see a few issues such as the SQFEET."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e3e3d",
   "metadata": {},
   "source": [
    "Once the data is combined and cleaned into one dataframe we may want to export the combined dataframe as a CSV file, explore the data and evaluate if it needs further cleaning before using for analysis or model development. For example, the last rows in the df_electricity_usage were from the template and can be dropped and not needed. We also need to explore any issues that the dataset may have such as null values, duplicates, other data issues, and creating derived features as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fc6f6a",
   "metadata": {},
   "source": [
    "# Reading Data Line By Line\n",
    "[Return to Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Many of the approaches below focus on reading data line by line. Loading and processing data line by line may be beneficial in some cases. This may be the case when a file may be too big to load locally in a computer or reading realtime data. In some cases, this approach may be more efficient (i.e., memory or processing perspective), make identifying error location easier, and allow reading a file in real-time. Note that in cases of large files other (better) options may include cloud environments where we may have more computational power at our disposal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59ab3e2",
   "metadata": {},
   "source": [
    "# Working with PDFs\n",
    "[Return to Table of Contents](#Table-of-Contents)\n",
    "\n",
    "There are various libraries that can read PDF's. Most common are PDFMiner and PyPDF2, however, none of the two are included in the Anaconda Python Distribution (potentially not as mature as other data science libraries). I have had the best experience converting the PDFs to HTML using Acrobat Pro (which requires a license) and then using an HTML Python library (like Beautifulsup) to extract the PDF text information which will include some information such as if the text is part of the heading, body, paragraph, etc. Other approaches may include converting to TXT and other type of files before processing. See the notebook '20_Working_with_WebData_and_API' for an example on working with PDF files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3906e76",
   "metadata": {},
   "source": [
    "# Python Built-In Functions for Loading Data (OPTIONAL)\n",
    "[Return to Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Python has various built-in functions to work with files. This sections discusses the open function which is used to Open a file and return the corresponding file object. The function has various parameters including the file directory and mode. The mode specifies the function (e.g., reading, writing). The following are the options for the mode:\n",
    "- 'r': open for reading (default)\n",
    "- 'w': open for writing, truncating the file first\n",
    "- 'x': open for exclusive creation, failing if the file already exists\n",
    "- 'a': open for writing, appending to the end of file if it exists\n",
    "- 'b' binary mode\n",
    "- 't' text mode (default)\n",
    "- '+' open for updating (reading and writing)\n",
    "\n",
    "Documentation References:\n",
    "- List of Python Built-in Functions https://docs.python.org/3/library/functions.html\n",
    "- Open Function: https://docs.python.org/3/library/functions.html#open"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0896ba9",
   "metadata": {},
   "source": [
    "## Open Function\n",
    "[Return to Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aff91af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tSpent Grain Bread\n",
      "\n",
      "\n",
      "\n",
      "\tI got this recipe from the 1985 Grain Brewing Issue of Zymurgy magazine.  I tried it as is, and wit bananas added and found it to make a good heavy bread.  The original article is by Clifford T. Newmn Jr. and he requests any original recipies to be sent to him at P.O. Box 193, Port Matilda, PA  1680\n",
      "\n",
      "\n",
      "\n",
      "\t-4 C. fresh spent grains (from your latest batch of \n",
      "\n",
      "\t-1 C. water               all grain beer)\n",
      "\n",
      "\t-1/2 C. oil\n",
      "\n",
      "\t-1/2 C. sugar\n",
      "\n",
      "\t-1/4 tsp. salt\n",
      "\n",
      "\t-1 Tbsp. dry baker's yeast\n",
      "\n",
      "\t-All-purpose flour (enough to make a stiff dough)\n",
      "\n",
      "\n",
      "\n",
      "\tPlace the spent grains and water into a blender or food processor and blend themn for 30 seconds. Ten put the blended grains in a large mixing bowl and add oil, sugar, salt and stir in the yeast. Addflour until you have a thick, workable dough. Put in a warm place to rise until doubled in size. The knead the dough and divide into three greased laof pans. Let the dough double in size again then bae in a preheated oven at 350 degrees for one hour and 15 minutes. Remove from oven and let cool on wre racks. Enjoy.\n",
      "\n",
      "\n",
      "\n",
      "\tFor storing grains until ready to make the bread, place them one half inch deep in a shallow bakingdish or cookie sheet and put them into the oven at 200 degrees. Stir the grains about every half hou until they feel dry. Then store in tightly sealed containers. If you see any drops of moisture in te containers it means the grains need to be dried longer.\n",
      "\n",
      "\n",
      "\n",
      "\tOnce dried, the grains will keep for a long time. They can be used dried or moist (fresh), and drie grains can be ground into flour. The recipe above is for moist (fresh) grains and if you are using ried grains, add on half cup of water for every four cups of grains in the recipe.\n",
      "\n",
      "\n",
      "\n",
      "\tI haven't tried the following recipe (yet) and am thinking of using wheat flour or flour from grain instead of white flour in my next batch of bread.\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tSpent Grains Granola\n",
      "\n",
      "\n",
      "\n",
      "\t-6 C. fresh spent grains\n",
      "\n",
      "\t-4 C. raw sunflour seeds\n",
      "\n",
      "\t-2 C. weat germ\n",
      "\n",
      "\t-1 C. bran\n",
      "\n",
      "\t-1 1/2 C. non-fat dry milk\n",
      "\n",
      "\t-2 tsp. salt\n",
      "\n",
      "\t-3 Tbsp. cinnamon\n",
      "\n",
      "\t-3/4 C. molasses\n",
      "\n",
      "\t-1/2 C. honey\n",
      "\n",
      "\t-3 Tbsp. vanilla extract\n",
      "\n",
      "\t-2 C. each--coconut, raisins, chopped cashews\n",
      "\n",
      "\n",
      "\n",
      "\tCombine first seven ingredients in a large bowl and mix well. Blend molasses, honey and vanilla in  small bowl, then add to the grains, mixing well. Spreat 1/2 inch deep on a large baking pan and bak at 250 degrees until light brown, stirring occasionally. Turn onto paper-covered surface. Mix last hree ingredients and combine with granola. Store in tightly sealed containers.0\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "X-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-X\n",
      "\n",
      "\n",
      "\n",
      " Another file downloaded from:                               NIRVANAnet(tm)\n",
      "\n",
      "\n",
      "\n",
      " & the Temple of the Screaming Electron   Jeff Hunter          510-935-5845\n",
      "\n",
      " Rat Head                                 Ratsnatcher          510-524-3649\n",
      "\n",
      " Burn This Flag                           Zardoz               408-363-9766\n",
      "\n",
      " realitycheck                             Poindexter Fortran   415-567-7043\n",
      "\n",
      " Lies Unlimited                           Mick Freen           415-583-4102\n",
      "\n",
      "\n",
      "\n",
      "   Specializing in conversations, obscure information, high explosives,\n",
      "\n",
      "       arcane knowledge, political extremism, diversive sexuality,\n",
      "\n",
      "       insane speculation, and wild rumours. ALL-TEXT BBS SYSTEMS.\n",
      "\n",
      "\n",
      "\n",
      "  Full access for first-time callers.  We don't want to know who you are,\n",
      "\n",
      "   where you live, or what your phone number is. We are not Big Brother.\n",
      "\n",
      "\n",
      "\n",
      "                          \"Raw Data for Raw Nerves\"\n",
      "\n",
      "\n",
      "\n",
      "X-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-X\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(file = \"./input_data/bread.txt\", mode = 'r')\n",
    "for line in f:\n",
    "    print(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f23ee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file at output_data/anotherfile.txt and delete the content.\n",
    "\n",
    "a = []\n",
    "for i in range(10):\n",
    "    a.append(\"All work and no play makes Jack a dull boy. \\nSecond Line. \\n\");\n",
    "f = open(\"./output_data/anotherfile.txt\", 'w')\n",
    "for line in a:\n",
    "    f.write(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8204a420",
   "metadata": {},
   "source": [
    "## With Statement\n",
    "[Return to Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The \"with\" keyword sets up a Context Manager, which temporarily deals with how the code runs. In this case, it closes the file automatically when the clause is left. \n",
    "\n",
    "Documentation References:\n",
    "- With Statement: https://docs.python.org/3/reference/compound_stmts.html#the-with-statement\n",
    "- Compound statements: https://docs.python.org/3/reference/compound_stmts.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ab3c5b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t\\t\\t\\tSpent Grain Bread\\n', '\\n', '\\tI got this recipe from the 1985 Grain Brewing Issue of Zymurgy magazine.  I tried it as is, and wit bananas added and found it to make a good heavy bread.  The original article is by Clifford T. Newmn Jr. and he requests any original recipies to be sent to him at P.O. Box 193, Port Matilda, PA  1680\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# Same as before but using the breadd.txt file and list comprehensions and printing only up to line 4.\n",
    "lines = []\n",
    "\n",
    "with open('./input_data/bread.txt') as f:\n",
    "    lines = [line for line in f]\n",
    "print(lines[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "939601ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines) # Still returns a list object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f39bbbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spent Grain Bread', '', 'I got this recipe from the 1985 Grain Brewing Issue of Zymurgy magazine.  I tried it as is, and wit bananas added and found it to make a good heavy bread.  The original article is by Clifford T. Newmn Jr. and he requests any original recipies to be sent to him at P.O. Box 193, Port Matilda, PA  1680', '']\n"
     ]
    }
   ],
   "source": [
    "# To remove the new line characters (\\n) can use a .rstrip() function within the for loop.\n",
    "# To remove the tab character (\\t) can use a .lstrip() function for loop.\n",
    "# To remove both can use the strip in the for loop.\n",
    "lines = []\n",
    "\n",
    "with open('./input_data/bread.txt') as f:\n",
    "    lines = [line.strip() for line in f]\n",
    "\n",
    "print(lines[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d5032",
   "metadata": {},
   "source": [
    "#### Example with Numerical Data in a TXT File\n",
    "[Return to Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d794cf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.1101,17.592\n",
      "\n",
      "5.5277,9.1302\n",
      "\n",
      "8.5186,13.662\n",
      "\n",
      "7.0032,11.854\n",
      "\n",
      "5.8598,6.8233\n",
      "\n",
      "8.3829,11.886\n",
      "\n",
      "7.4764,4.3483\n",
      "\n",
      "8.5781,12\n",
      "\n",
      "6.4862,6.5987\n",
      "\n",
      "5.0546,3.8166\n",
      "\n",
      "5.7107,3.2522\n",
      "\n",
      "14.164,15.505\n",
      "\n",
      "5.734,3.1551\n",
      "\n",
      "8.4084,7.2258\n",
      "\n",
      "5.6407,0.71618\n",
      "\n",
      "5.3794,3.5129\n",
      "\n",
      "6.3654,5.3048\n",
      "\n",
      "5.1301,0.56077\n",
      "\n",
      "6.4296,3.6518\n",
      "\n",
      "7.0708,5.3893\n",
      "\n",
      "6.1891,3.1386\n",
      "\n",
      "20.27,21.767\n",
      "\n",
      "5.4901,4.263\n",
      "\n",
      "6.3261,5.1875\n",
      "\n",
      "5.5649,3.0825\n",
      "\n",
      "18.945,22.638\n",
      "\n",
      "12.828,13.501\n",
      "\n",
      "10.957,7.0467\n",
      "\n",
      "13.176,14.692\n",
      "\n",
      "22.203,24.147\n",
      "\n",
      "5.2524,-1.22\n",
      "\n",
      "6.5894,5.9966\n",
      "\n",
      "9.2482,12.134\n",
      "\n",
      "5.8918,1.8495\n",
      "\n",
      "8.2111,6.5426\n",
      "\n",
      "7.9334,4.5623\n",
      "\n",
      "8.0959,4.1164\n",
      "\n",
      "5.6063,3.3928\n",
      "\n",
      "12.836,10.117\n",
      "\n",
      "6.3534,5.4974\n",
      "\n",
      "5.4069,0.55657\n",
      "\n",
      "6.8825,3.9115\n",
      "\n",
      "11.708,5.3854\n",
      "\n",
      "5.7737,2.4406\n",
      "\n",
      "7.8247,6.7318\n",
      "\n",
      "7.0931,1.0463\n",
      "\n",
      "5.0702,5.1337\n",
      "\n",
      "5.8014,1.844\n",
      "\n",
      "11.7,8.0043\n",
      "\n",
      "5.5416,1.0179\n",
      "\n",
      "7.5402,6.7504\n",
      "\n",
      "5.3077,1.8396\n",
      "\n",
      "7.4239,4.2885\n",
      "\n",
      "7.6031,4.9981\n",
      "\n",
      "6.3328,1.4233\n",
      "\n",
      "6.3589,-1.4211\n",
      "\n",
      "6.2742,2.4756\n",
      "\n",
      "5.6397,4.6042\n",
      "\n",
      "9.3102,3.9624\n",
      "\n",
      "9.4536,5.4141\n",
      "\n",
      "8.8254,5.1694\n",
      "\n",
      "5.1793,-0.74279\n",
      "\n",
      "21.279,17.929\n",
      "\n",
      "14.908,12.054\n",
      "\n",
      "18.959,17.054\n",
      "\n",
      "7.2182,4.8852\n",
      "\n",
      "8.2951,5.7442\n",
      "\n",
      "10.236,7.7754\n",
      "\n",
      "5.4994,1.0173\n",
      "\n",
      "20.341,20.992\n",
      "\n",
      "10.136,6.6799\n",
      "\n",
      "7.3345,4.0259\n",
      "\n",
      "6.0062,1.2784\n",
      "\n",
      "7.2259,3.3411\n",
      "\n",
      "5.0269,-2.6807\n",
      "\n",
      "6.5479,0.29678\n",
      "\n",
      "7.5386,3.8845\n",
      "\n",
      "5.0365,5.7014\n",
      "\n",
      "10.274,6.7526\n",
      "\n",
      "5.1077,2.0576\n",
      "\n",
      "5.7292,0.47953\n",
      "\n",
      "5.1884,0.20421\n",
      "\n",
      "6.3557,0.67861\n",
      "\n",
      "9.7687,7.5435\n",
      "\n",
      "6.5159,5.3436\n",
      "\n",
      "8.5172,4.2415\n",
      "\n",
      "9.1802,6.7981\n",
      "\n",
      "6.002,0.92695\n",
      "\n",
      "5.5204,0.152\n",
      "\n",
      "5.0594,2.8214\n",
      "\n",
      "5.7077,1.8451\n",
      "\n",
      "7.6366,4.2959\n",
      "\n",
      "5.8707,7.2029\n",
      "\n",
      "5.3054,1.9869\n",
      "\n",
      "8.2934,0.14454\n",
      "\n",
      "13.394,9.0551\n",
      "\n",
      "5.4369,0.61705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"./input_data/ex1data1.txt\") as f:\n",
    "    for line in f:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "42083848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(line) # Note that the line variable is a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4e70949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6.1101,17.592\\n', '5.5277,9.1302\\n', '8.5186,13.662\\n', '7.0032,11.854\\n', '5.8598,6.8233\\n', '8.3829,11.886\\n', '7.4764,4.3483\\n', '8.5781,12\\n', '6.4862,6.5987\\n', '5.0546,3.8166\\n', '5.7107,3.2522\\n', '14.164,15.505\\n', '5.734,3.1551\\n', '8.4084,7.2258\\n', '5.6407,0.71618\\n', '5.3794,3.5129\\n', '6.3654,5.3048\\n', '5.1301,0.56077\\n', '6.4296,3.6518\\n', '7.0708,5.3893\\n', '6.1891,3.1386\\n', '20.27,21.767\\n', '5.4901,4.263\\n', '6.3261,5.1875\\n', '5.5649,3.0825\\n', '18.945,22.638\\n', '12.828,13.501\\n', '10.957,7.0467\\n', '13.176,14.692\\n', '22.203,24.147\\n', '5.2524,-1.22\\n', '6.5894,5.9966\\n', '9.2482,12.134\\n', '5.8918,1.8495\\n', '8.2111,6.5426\\n', '7.9334,4.5623\\n', '8.0959,4.1164\\n', '5.6063,3.3928\\n', '12.836,10.117\\n', '6.3534,5.4974\\n', '5.4069,0.55657\\n', '6.8825,3.9115\\n', '11.708,5.3854\\n', '5.7737,2.4406\\n', '7.8247,6.7318\\n', '7.0931,1.0463\\n', '5.0702,5.1337\\n', '5.8014,1.844\\n', '11.7,8.0043\\n', '5.5416,1.0179\\n', '7.5402,6.7504\\n', '5.3077,1.8396\\n', '7.4239,4.2885\\n', '7.6031,4.9981\\n', '6.3328,1.4233\\n', '6.3589,-1.4211\\n', '6.2742,2.4756\\n', '5.6397,4.6042\\n', '9.3102,3.9624\\n', '9.4536,5.4141\\n', '8.8254,5.1694\\n', '5.1793,-0.74279\\n', '21.279,17.929\\n', '14.908,12.054\\n', '18.959,17.054\\n', '7.2182,4.8852\\n', '8.2951,5.7442\\n', '10.236,7.7754\\n', '5.4994,1.0173\\n', '20.341,20.992\\n', '10.136,6.6799\\n', '7.3345,4.0259\\n', '6.0062,1.2784\\n', '7.2259,3.3411\\n', '5.0269,-2.6807\\n', '6.5479,0.29678\\n', '7.5386,3.8845\\n', '5.0365,5.7014\\n', '10.274,6.7526\\n', '5.1077,2.0576\\n', '5.7292,0.47953\\n', '5.1884,0.20421\\n', '6.3557,0.67861\\n', '9.7687,7.5435\\n', '6.5159,5.3436\\n', '8.5172,4.2415\\n', '9.1802,6.7981\\n', '6.002,0.92695\\n', '5.5204,0.152\\n', '5.0594,2.8214\\n', '5.7077,1.8451\\n', '7.6366,4.2959\\n', '5.8707,7.2029\\n', '5.3054,1.9869\\n', '8.2934,0.14454\\n', '13.394,9.0551\\n', '5.4369,0.61705\\n']\n"
     ]
    }
   ],
   "source": [
    "lines = [] # Initializing an empty list.\n",
    "\n",
    "with open(\"./input_data/ex1data1.txt\") as f:\n",
    "    for line in f:\n",
    "        lines.append(line)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2c7ced2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines) # Notes that in this case the lines are a list and can be accessed by element."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf58d9",
   "metadata": {},
   "source": [
    "In this case where each line is two columns of data we may need to use a different approach. \n",
    "\n",
    "We can use the split function to separate the nubmers by the comma and may also need to change the string to a number (i.e., float in this case). Could later use Pandas and transform the data to a dataframe.\n",
    "\n",
    "Continue to the OS Library below to see how to process the data in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e777974b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.1101, 17.592], [5.5277, 9.1302], [8.5186, 13.662], [7.0032, 11.854], [5.8598, 6.8233], [8.3829, 11.886], [7.4764, 4.3483], [8.5781, 12.0], [6.4862, 6.5987], [5.0546, 3.8166], [5.7107, 3.2522], [14.164, 15.505], [5.734, 3.1551], [8.4084, 7.2258], [5.6407, 0.71618], [5.3794, 3.5129], [6.3654, 5.3048], [5.1301, 0.56077], [6.4296, 3.6518], [7.0708, 5.3893], [6.1891, 3.1386], [20.27, 21.767], [5.4901, 4.263], [6.3261, 5.1875], [5.5649, 3.0825], [18.945, 22.638], [12.828, 13.501], [10.957, 7.0467], [13.176, 14.692], [22.203, 24.147], [5.2524, -1.22], [6.5894, 5.9966], [9.2482, 12.134], [5.8918, 1.8495], [8.2111, 6.5426], [7.9334, 4.5623], [8.0959, 4.1164], [5.6063, 3.3928], [12.836, 10.117], [6.3534, 5.4974], [5.4069, 0.55657], [6.8825, 3.9115], [11.708, 5.3854], [5.7737, 2.4406], [7.8247, 6.7318], [7.0931, 1.0463], [5.0702, 5.1337], [5.8014, 1.844], [11.7, 8.0043], [5.5416, 1.0179], [7.5402, 6.7504], [5.3077, 1.8396], [7.4239, 4.2885], [7.6031, 4.9981], [6.3328, 1.4233], [6.3589, -1.4211], [6.2742, 2.4756], [5.6397, 4.6042], [9.3102, 3.9624], [9.4536, 5.4141], [8.8254, 5.1694], [5.1793, -0.74279], [21.279, 17.929], [14.908, 12.054], [18.959, 17.054], [7.2182, 4.8852], [8.2951, 5.7442], [10.236, 7.7754], [5.4994, 1.0173], [20.341, 20.992], [10.136, 6.6799], [7.3345, 4.0259], [6.0062, 1.2784], [7.2259, 3.3411], [5.0269, -2.6807], [6.5479, 0.29678], [7.5386, 3.8845], [5.0365, 5.7014], [10.274, 6.7526], [5.1077, 2.0576], [5.7292, 0.47953], [5.1884, 0.20421], [6.3557, 0.67861], [9.7687, 7.5435], [6.5159, 5.3436], [8.5172, 4.2415], [9.1802, 6.7981], [6.002, 0.92695], [5.5204, 0.152], [5.0594, 2.8214], [5.7077, 1.8451], [7.6366, 4.2959], [5.8707, 7.2029], [5.3054, 1.9869], [8.2934, 0.14454], [13.394, 9.0551], [5.4369, 0.61705]]\n"
     ]
    }
   ],
   "source": [
    "f = open(\"./input_data/ex1data1.txt\")\n",
    "data = []\n",
    "\n",
    "for line in f:\n",
    "    parsed_line = str.split(line,\",\") # Line or row of data in this example.\n",
    "    data_line = []\n",
    "    for element in parsed_line:\n",
    "        data_line.append(float(element))\n",
    "    data.append(data_line)\n",
    "print(data)\n",
    "f.close()\n",
    "\n",
    "# This produces a list within list where the inner list is the two columsn of data and the outer is the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab06d2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "830d896c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) # Think number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fb3eb22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0]) # Think number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f043f3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[0]) # Row of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "90f22d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "6.1101\n"
     ]
    }
   ],
   "source": [
    "print(type(data[0][0])) # Element within row of data.\n",
    "print(data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e91d9ea",
   "metadata": {},
   "source": [
    "# JSON Library: JSON Files (OPTIONAL) \n",
    "[Return to Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In previous lectures we have explored how to export dictionary (i.e., JSON) to a Pandas Dataframes and viceversa. This section provide furhter examples on how to work with JSON data using the JSON library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1c7f7a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing JSON to a FILE\n",
    "data = {}  \n",
    "data['people'] = []  \n",
    "data['people'].append({  \n",
    "    'name': 'Scott',\n",
    "    'website': 'umbc.edu',\n",
    "    'from': 'Maryland'\n",
    "})\n",
    "data['people'].append({  \n",
    "    'name': 'Larry',\n",
    "    'website': 'google.com',\n",
    "    'from': 'Michigan'\n",
    "})\n",
    "data['people'].append({  \n",
    "    'name': 'Tim',\n",
    "    'website': 'apple.com',\n",
    "    'from': 'Alabama'\n",
    "})\n",
    "#!mkdir outputs\n",
    "with open('./output_data/json_data.txt', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "005efeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Scott\n",
      "Website: umbc.edu\n",
      "From: Maryland\n",
      "\n",
      "Name: Larry\n",
      "Website: google.com\n",
      "From: Michigan\n",
      "\n",
      "Name: Tim\n",
      "Website: apple.com\n",
      "From: Alabama\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading JSON from a TXT File\n",
    "with open('./output_data/json_data.txt') as json_file:  \n",
    "    data = json.load(json_file)\n",
    "    for p in data['people']:\n",
    "        print('Name: ' + p['name'])\n",
    "        print('Website: ' + p['website'])\n",
    "        print('From: ' + p['from'])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1810286a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "96b69473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'FeatureCollection', 'features': [{'type': 'Feature', 'geometry': {'type': 'Point', 'coordinates': [42.0, 21.0]}, 'properties': {'prop0': 'value0'}}]}\n",
      "____________________________________\n",
      "[{'type': 'Feature', 'geometry': {'type': 'Point', 'coordinates': [42.0, 21.0]}, 'properties': {'prop0': 'value0'}}]\n",
      "____________________________________\n",
      "{'type': 'Point', 'coordinates': [42.0, 21.0]}\n",
      "____________________________________\n",
      "42.0\n"
     ]
    }
   ],
   "source": [
    "# JSON Read\n",
    "f = open('./input_data/geo_data.json')\n",
    "data = json.load(f)\n",
    "f.close()    \n",
    "print(data)\n",
    "print('____________________________________')\n",
    "print(data[\"features\"])\n",
    "print('____________________________________')\n",
    "print(data[\"features\"][0][\"geometry\"])\n",
    "print('____________________________________')\n",
    "for i in data[\"features\"]:\n",
    "    print(i[\"geometry\"][\"coordinates\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2be82b6",
   "metadata": {},
   "source": [
    "# Working with CSV Files (CSV Library)\n",
    "[Return to Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Note that previously we have learned to work with CSV data using the Pandas library and the Pandas library is the prefered approach. However, the CSV library exists and can be usefule too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1318dfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,10,50,50,10, \"12\", A\n",
      "10,50,50,10,10, \"12\", \"A\"\n",
      "25,25,75,75,25, \"12\", \"A\"\n",
      "25,75,75,25,25, \"12\", \"A\"\n",
      "50,50,100,100,50, \"12\", \"A\"\n",
      "50,100,100,50,50, \"12\", \"A\",6\n"
     ]
    }
   ],
   "source": [
    "f = open('./input_data/csv_plain_file.csv', newline='') \n",
    "reader = csv.reader(f, quoting = csv.QUOTE_NONNUMERIC)\n",
    "for row in reader: # A list of rows\n",
    "    for value in row: # A list of value\n",
    "        print(value) # Floats\n",
    "f.close() # Don't close until you are done with the reader;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6e605130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'Jack', 'John \"Da Man\"', 'Stephen', '', 'Joan \"the bone\", Anne']\n",
      "['Doe', 'McGinnis', 'Repici', 'Tyler', 'Blankman', 'Jet']\n",
      "[' 08075', '09119', '08075', ' 91234', ' 00298', '00123']\n"
     ]
    }
   ],
   "source": [
    "with open('./input_data/addresses.csv') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    firstnames, lastnames, streets, citys, states,zipcodes = [], [], [],[], [], []\n",
    "        \n",
    "    for row in readCSV:\n",
    "        firstname, lastname, street = row[0], row[1], row[2] \n",
    "        city, state, zipcode  = row[3], row[4], row[5]  \n",
    "\n",
    "        firstnames.append(firstname)\n",
    "        lastnames.append(lastname)\n",
    "        zipcodes.append(zipcode)\n",
    "\n",
    "    print(firstnames)\n",
    "    print(lastnames)\n",
    "    print(zipcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0d9a41ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('John', 'Doe', ' NJ')\n",
      "('Jack', 'McGinnis', ' PA')\n",
      "('John \"Da Man\"', 'Repici', ' NJ')\n",
      "('Stephen', 'Tyler', 'SD')\n",
      "('', 'Blankman', ' SD')\n",
      "('Joan \"the bone\", Anne', 'Jet', 'CO')\n"
     ]
    }
   ],
   "source": [
    "# let's create a new csv file from an existing one\n",
    "# New file will contain only the information that we want/need\n",
    "f2 = open('./output_data/dataout1.csv', 'w', newline='') \n",
    "writer = csv.writer(f2, delimiter=',')\n",
    "\n",
    "with open('./input_data/addresses.csv') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    firstnames, lastnames, streets, citys, states,zipcodes = [], [], [],[], [], []\n",
    "        \n",
    "    for row in readCSV:\n",
    "        firstname, lastname, street = row[0], row[1], row[2] \n",
    "        city, state, zipcode  = row[3], row[4], row[5]\n",
    "        data2write = (firstname,lastname,state)\n",
    "        print(data2write)\n",
    "        writer.writerow(data2write)\n",
    "        \n",
    "    f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "61fe4916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John lives in Riverside,  NJ\n",
      "Jack lives in Phila,  PA\n",
      "John \"Da Man\" lives in Riverside,  NJ\n",
      "Stephen lives in SomeTown, SD\n",
      " lives in SomeTown,  SD\n",
      "Joan \"the bone\", Anne lives in Desert City, CO\n"
     ]
    }
   ],
   "source": [
    "# Let's read csv and write txt\n",
    "\n",
    "with open('./input_data/addresses.csv') as csvfile:\n",
    "    f2 = open(\"./output_data/textout.txt\", \"w\")\n",
    "\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    firstnames, lastnames, streets, citys, states,zipcodes = [], [], [],[], [], []\n",
    "        \n",
    "    for row in readCSV:\n",
    "        firstname, lastname, street = row[0], row[1], row[2] \n",
    "        city, state, zipcode  = row[3], row[4], row[5]\n",
    "        text2write = firstname+' lives in '+ city +', '+state\n",
    "        print(text2write)\n",
    "        f2.write(text2write+'\\n')\n",
    "        \n",
    "    f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5c4391",
   "metadata": {},
   "source": [
    "# Reading Multiple TXT Files (OPTIONAL)\n",
    "[Return to Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3bb55838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./input_data/text_files/example3.txt\n",
      "Xanthomonas\tcampestris\thyacinthi\tLMG739\t1\t5\t8\t4\t3\t2\t4\t1\n",
      "\n",
      "./input_data/text_files/example3.txt\n",
      "Xanthomonas\tcampestris\tcampestris\tLMG568\t3\t5\t6\t2\t3\t1\t4\t3\n",
      "\n",
      "./input_data/text_files/example3.txt\n",
      "Xanthomonas\tcampestris\tphaseoli\tLMG803\t2\t4\t7\t4\t1\t2\t3\t2\n",
      "\n",
      "./input_data/text_files/example3.txt\n",
      "Xanthomonas\tcampestris\thyacinthi\tLMG946\t2\t3\t6\t2\t1\t0\t1\t0\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "Sample file\tBand no.\tRun time\tSize\tHeight\tArea\tRun time\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t1\t35.40\t70.43\t216\t1111\t354\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t2\t36.90\t76.86\t252\t1318\t369\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t3\t55.40\t154.72\t717\t5726\t554\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t4\t62.20\t183.46\t120\t1520\t622\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t5\t70.00\t215.72\t1644\t12165\t700\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t6\t78.40\t251.33\t1113\t6840\t784\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t7\t94.40\t324.17\t196\t1141\t944\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t8\t95.20\t327.98\t110\t730\t952\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t9\t97.50\t339.02\t130\t678\t975\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t10\t101.80\t360.01\t101\t387\t1018\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t11\t109.30\t398.24\t437\t4151\t1093\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t12\t112.80\t416.93\t145\t1511\t1128\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t13\t121.30\t464.73\t160\t1004\t1213\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t14\t122.10\t469.41\t175\t1186\t1221\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t15\t135.40\t552.17\t113\t512\t1354\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t16\t136.80\t561.41\t150\t876\t1368\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t17\t139.70\t581.00\t231\t1585\t1397\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t18\t146.60\t630.13\t281\t1580\t1466\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t19\t153.00\t679.18\t419\t3926\t1530\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t20\t156.90\t710.89\t207\t1082\t1569\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t21\t163.70\t769.81\t203\t1377\t1637\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t22\t169.20\t821.18\t197\t1379\t1692\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "15B\t23\t210.70\t\t239\t2766\t2107\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t1\t30.90\t50.32\t120\t557\t309\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t2\t35.30\t69.74\t363\t1946\t353\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t3\t36.80\t76.35\t169\t1108\t368\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t4\t50.50\t134.62\t170\t2053\t505\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t5\t55.30\t154.84\t955\t8872\t553\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t6\t64.40\t192.97\t100\t538\t644\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t7\t67.80\t207.13\t103\t953\t678\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t8\t69.70\t215.16\t794\t6394\t697\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t9\t78.10\t251.08\t1960\t13690\t781\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t10\t81.60\t266.33\t166\t1317\t816\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t11\t97.30\t339.15\t128\t771\t973\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t12\t101.60\t360.02\t131\t478\t1016\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t13\t109.00\t397.88\t233\t2109\t1090\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t14\t121.00\t464.69\t137\t1316\t1210\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t15\t136.50\t561.48\t314\t2020\t1365\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t16\t139.30\t580.42\t269\t2086\t1393\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t17\t152.70\t679.60\t180\t1611\t1527\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t18\t161.50\t753.53\t151\t925\t1615\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t19\t162.10\t758.86\t181\t1181\t1621\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t20\t163.30\t769.64\t192\t1580\t1633\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "16B\t21\t168.80\t821.16\t207\t1467\t1688\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "17B\t1\t33.00\t60.47\t228\t929\t330\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "17B\t2\t34.60\t67.53\t201\t815\t346\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "17B\t3\t43.30\t106.02\t113\t855\t433\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "17B\t4\t52.90\t146.14\t381\t1908\t529\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "17B\t5\t88.20\t298.95\t131\t690\t882\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "17B\t6\t89.00\t302.68\t1425\t7821\t890\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "17B\t7\t155.40\t709.46\t304\t1800\t1554\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "17B\t8\t158.50\t736.00\t182\t966\t1585\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "17B\t9\t165.10\t796.02\t121\t713\t1651\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t1\t33.10\t61.79\t197\t1308\t331\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t2\t34.90\t69.74\t122\t672\t349\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t3\t35.80\t73.71\t181\t1062\t358\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t4\t38.30\t84.74\t182\t902\t383\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t5\t54.80\t155.01\t590\t3707\t548\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t6\t60.20\t177.94\t683\t3513\t602\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t7\t69.10\t215.57\t440\t2731\t691\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t8\t76.60\t247.61\t121\t651\t766\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t9\t77.70\t252.30\t312\t1761\t777\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t10\t93.20\t323.82\t1245\t6175\t932\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t11\t94.10\t328.10\t261\t2106\t941\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t12\t107.90\t397.79\t1319\t11553\t1079\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t13\t120.70\t470.00\t802\t4926\t1207\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t14\t121.90\t476.99\t103\t995\t1219\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t15\t132.70\t544.46\t278\t2850\t1327\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t16\t133.90\t552.38\t144\t873\t1339\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t17\t138.10\t580.89\t109\t734\t1381\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t18\t142.40\t611.44\t121\t620\t1424\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t19\t143.00\t615.81\t179\t1493\t1430\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t20\t143.80\t621.69\t126\t744\t1438\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t21\t151.90\t684.31\t102\t921\t1519\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t22\t153.00\t693.27\t128\t923\t1530\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t23\t161.30\t764.75\t115\t742\t1613\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t24\t162.10\t772.03\t118\t751\t1621\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "18B\t25\t168.80\t835.74\t107\t712\t1688\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t1\t31.10\t53.41\t100\t483\t311\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t2\t33.00\t61.79\t175\t785\t330\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t3\t34.90\t70.18\t1758\t10330\t349\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t4\t36.40\t76.79\t326\t3854\t364\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t5\t54.60\t154.87\t1066\t9171\t546\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t6\t59.90\t177.52\t145\t669\t599\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t7\t61.50\t184.30\t117\t837\t615\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t8\t63.70\t193.54\t143\t755\t637\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t9\t67.00\t207.48\t177\t679\t670\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t10\t67.50\t209.60\t236\t1682\t675\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t11\t68.90\t215.57\t1094\t8775\t689\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t12\t74.80\t240.96\t173\t1034\t748\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t13\t76.40\t247.79\t109\t565\t764\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t14\t77.20\t251.26\t1512\t9193\t772\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t15\t80.70\t266.74\t346\t2195\t807\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t16\t88.70\t303.53\t128\t603\t887\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t17\t96.20\t339.28\t441\t3113\t962\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t18\t100.40\t360.00\t181\t638\t1004\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t19\t103.10\t373.81\t197\t1021\t1031\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t20\t107.20\t395.38\t308\t1465\t1072\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t21\t107.70\t398.06\t382\t3056\t1077\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t22\t111.00\t416.03\t902\t11417\t1110\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t23\t119.50\t464.64\t383\t3482\t1195\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t24\t134.90\t561.38\t443\t3683\t1349\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t25\t137.70\t580.27\t226\t1474\t1377\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t26\t142.40\t613.30\t102\t1051\t1424\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t27\t144.50\t628.63\t273\t1972\t1445\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t28\t150.80\t676.89\t1033\t11980\t1508\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t29\t154.70\t708.60\t431\t3469\t1547\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t30\t159.50\t749.75\t441\t2729\t1595\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t31\t160.20\t755.95\t130\t1061\t1602\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t32\t161.30\t765.81\t165\t1384\t1613\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t33\t166.90\t818.22\t508\t4040\t1669\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t34\t168.10\t829.98\t102\t973\t1681\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t35\t181.70\t981.57\t106\t1363\t1817\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t36\t182.90\t996.68\t307\t3390\t1829\n",
      "\n",
      "./input_data/text_files/Genescan.txt\n",
      "19B\t37\t208.00\t\t113\t1881\t2080\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "_\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "                                                                                          DATA2:F90705633     06-JAN-95 16:29:38\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "ID:     4632       XANT-ALBIL (LMG 482)                                           Date of run: 06-JUL-90 03:16:41\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "Bottle: 18         SAMPLE      [AEROBE]                                           Date edited: 05-OCT-93 18:09:08\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "   RT      Area    Ar/Ht Respon     ECL           Name            %         Comment 1             Comment 2      \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "------- --------- ------ ------   ------  -------------------- ------  --------------------  --------------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  1.733  31287000  0.062  . . .    6.997  SOLVENT PEAK . . . .  . . .  < min rt                                  \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  3.131      1399  0.033  1.210    9.999  10:0 . . . . . . . .   0.89  ECL deviates -0.001   Reference  0.002    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  3.522      9527  0.036  1.173   10.606  11:0 ISO . . . . . .   5.85  ECL deviates  0.001   Reference  0.004    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  4.724      5540  0.041  1.093   12.086  11:0 ISO 3OH . . . .   3.17  ECL deviates -0.004                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  6.291      2791  0.050  1.030   13.449  12:0 3OH . . . . . .   1.50  ECL deviates -0.006                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  7.159      4777  0.051  1.004   14.106  13:0 ISO 3OH . . . .   2.51  ECL deviates -0.004                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  7.918      3182  0.052  0.985   14.621  15:0 ISO . . . . . .   1.64  ECL deviates  0.000   Reference  0.000    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  9.360      2525  0.061  0.955   15.550  16:0 N alcohol . . .   1.26  ECL deviates  0.001                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  9.482     10702  0.055  0.952   15.626  16:0 ISO . . . . . .   5.34  ECL deviates -0.000   Reference -0.001    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  9.788     26845  0.057  0.947   15.816  16:1 w7c . . . . . .  13.30  ECL deviates -0.001                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 10.082     30425  0.055  0.942   15.999  16:0 . . . . . . . .  14.99  ECL deviates -0.001   Reference -0.002    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 10.415      1486  0.056  . . .   16.197   . . . . . . . . . .  . . .                                            \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 10.787     48496  0.058  0.930   16.417  ISO 17:1 w9c . . . .  23.61  ECL deviates  0.001                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 11.144     35119  0.057  0.925   16.629  17:0 ISO . . . . . .  17.00  ECL deviates  0.000   Reference -0.001    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 11.301      8521  0.057  0.923   16.723  17:0 ANTEISO . . . .   4.11  ECL deviates  0.001   Reference -0.001    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 11.418      1096  0.056  0.921   16.792  17:1 w8c . . . . . .   0.53  ECL deviates  0.000                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 12.471      1525  0.068  0.907   17.408  17:0 10 methyl . . .   0.72  ECL deviates -0.002                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 13.185      6278  0.068  0.898   17.822  Sum In Feature 7 . .   2.95  ECL deviates -0.000   18:1 w7c/w9t/w12t   \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 13.767      1317  0.067  0.892   18.159  17:0 ISO 3OH . . . .   0.61  ECL deviates -0.005                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "*******      6278  . . .  . . .    . . .  SUMMED FEATURE 7 . .   2.95  18:1 w7c/w9t/w12t     18:1 w9c/w12t/w7c   \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "******* . . . . .  . . .  . . .    . . .   . . . . . . . . . .  . . .  18:1 w12t/w9t/w7c                         \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "Solvent Ar  Total Area  Named Area  % Named  Total Amnt  Nbr Ref  ECL Deviation  Ref ECL Shift\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "----------  ----------  ----------  -------  ----------  -------  -------------  -------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  31287000      201551      200065    99.26      191066        7          0.002          0.002\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "_\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "DATA2:F90423424     06-JAN-95 16:29:38\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "ID:     3698       XANT-ALBIL (LMG 494)                                           Date of run: 23-APR-90 18:50:07\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "Bottle: 62         SAMPLE      [AEROBE]\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "   RT      Area    Ar/Ht Respon     ECL           Name            %         Comment 1             Comment 2      \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "------- --------- ------ ------   ------  -------------------- ------  --------------------  --------------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  1.499  30248000  0.060  . . .    7.048  SOLVENT PEAK . . . .  . . .  < min rt                                  \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  2.880      1598  0.030  1.163    9.999  10:0 . . . . . . . .   0.83  ECL deviates -0.001   Reference -0.004    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  3.273     10592  0.031  1.131   10.606  11:0 ISO . . . . . .   5.37  ECL deviates  0.001   Reference -0.003    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  4.486      7303  0.037  1.065   12.084  11:0 ISO 3OH . . . .   3.48  ECL deviates -0.006                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  6.078      2905  0.042  1.016   13.447  12:0 3OH . . . . . .   1.32  ECL deviates -0.008                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  6.958      5320  0.046  0.996   14.103  13:0 ISO 3OH . . . .   2.37  ECL deviates -0.007                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  7.732      3600  0.047  0.982   14.621  15:0 ISO . . . . . .   1.58  ECL deviates  0.000   Reference -0.001    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  8.297       970  0.047  0.973   15.000  15:0 . . . . . . . .   0.42  ECL deviates -0.000   Reference -0.002    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  9.314      5533  0.049  0.959   15.626  16:0 ISO . . . . . .   2.38  ECL deviates  0.000   Reference -0.002    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  9.619     28731  0.051  0.955   15.814  16:1 w7c . . . . . .  12.28  ECL deviates -0.003                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  9.921     35103  0.051  0.951   16.000  16:0 . . . . . . . .  14.95  ECL deviates -0.000   Reference -0.002    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 10.249      1840  0.053  . . .   16.193   . . . . . . . . . .  . . .                                            \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 10.624     61998  0.051  0.943   16.414  ISO 17:1 w9c . . . .  26.18  ECL deviates -0.002                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 10.721      2854  0.056  0.942   16.471  Sum In Feature 5 . .   1.20  ECL deviates -0.005   17:1 ISO I/ANTEI B  \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 10.991     42461  0.051  0.939   16.630  17:0 ISO . . . . . .  17.86  ECL deviates  0.001   Reference -0.001    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 11.147      7189  0.052  0.938   16.721  17:0 ANTEISO . . . .   3.02  ECL deviates -0.001   Reference -0.003    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 11.261      1629  0.053  0.937   16.788  17:1 w8c . . . . . .   0.68  ECL deviates -0.004                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 12.316       776  0.052  0.927   17.401  17:0 10 methyl . . .   0.32  ECL deviates -0.009                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 13.042      7728  0.059  0.921   17.819  Sum In Feature 7 . .   3.19  ECL deviates -0.003   18:1 w7c/w9t/w12t   \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 13.623      1555  0.057  0.916   18.154  17:0 ISO 3OH . . . .   0.64  ECL deviates -0.010                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 14.451      3576  0.055  0.911   18.633  19:0 ISO . . . . . .   1.46  ECL deviates -0.000   Reference -0.003    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 16.800      1159  0.065  0.897   20.000  20:0 . . . . . . . .   0.47  ECL deviates  0.000   Reference -0.004    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 17.866      2247  0.055  . . .   20.623   . . . . . . . . . .  . . .  > max rt                                  \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "*******      2854  . . .  . . .    . . .  SUMMED FEATURE 5 . .   1.20  17:1 ISO I/ANTEI B    17:1 ANTEISO B/i I  \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "*******      7728  . . .  . . .    . . .  SUMMED FEATURE 7 . .   3.19  18:1 w7c/w9t/w12t     18:1 w9c/w12t/w7c   \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "******* . . . . .  . . .  . . .    . . .   . . . . . . . . . .  . . .  18:1 w12t/w9t/w7c                         \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "Solvent Ar  Total Area  Named Area  % Named  Total Amnt  Nbr Ref  ECL Deviation  Ref ECL Shift\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "----------  ----------  ----------  -------  ----------  -------  -------------  -------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  30248000      234420      232580    99.22      223342       10          0.004          0.003\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "_\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "                                                                                          DATA2:F92209640     11-JAN-95 15:30:33\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "ID:     12085      XANT-ARBOR-POPUL (LMG 12141)                                   Date of run: 09-FEB-92 22:51:07\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "Bottle: 11         SAMPLE      [AEROBE]                                           Date edited: 05-OCT-93 12:13:17\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "   RT      Area    Ar/Ht Respon     ECL           Name            %         Comment 1             Comment 2      \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "------- --------- ------ ------   ------  -------------------- ------  --------------------  --------------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  1.656  37515000  0.075  . . .    7.004  SOLVENT PEAK . . . .  . . .  < min rt                                  \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  3.096      2008  0.032  1.125    9.999  10:0 . . . . . . . .   0.73  ECL deviates -0.001   Reference  0.006    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  3.499     13875  0.033  1.099   10.607  11:0 ISO . . . . . .   4.95  ECL deviates  0.002   Reference  0.008    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  4.129       728  0.041  1.069   11.419  10:0 3OH . . . . . .   0.25  ECL deviates -0.004                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  4.461      6210  0.037  1.057   11.796  unknown 11.798 . . .   2.13  ECL deviates -0.002                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  4.736     10049  0.038  1.048   12.085  11:0 ISO 3OH . . . .   3.42  ECL deviates -0.005                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  5.122       870  0.055  1.038   12.434  11:0 3OH . . . . . .   0.29  ECL deviates -0.007                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  5.319      2241  0.060  1.034   12.612  13:0 ISO . . . . . .   0.75  ECL deviates -0.000   Reference  0.002    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  6.346      9035  0.043  1.013   13.450  12:0 3OH . . . . . .   2.97  ECL deviates -0.005                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  6.567      3826  0.043  1.010   13.617  14:0 ISO . . . . . .   1.25  ECL deviates -0.001   Reference -0.001    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  6.941      1297  0.045  1.004   13.899  Sum In Feature 1 . .   0.42  ECL deviates -0.004   14:1 w5c/14:1 w5t   \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  7.073      6004  0.043  1.002   13.999  14:0 . . . . . . . .   1.95  ECL deviates -0.001   Reference -0.002    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  7.233     14999  0.044  1.000   14.105  13:0 ISO 3OH . . . .   4.87  ECL deviates -0.005                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  7.358      2022  0.045  0.998   14.189  13:0 2OH . . . . . .   0.65  ECL deviates -0.002                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  7.694      2681  0.046  0.994   14.413  15:1 ISO F . . . . .   0.86  ECL deviates -0.001                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  8.009    105150  0.045  0.990   14.623  15:0 ISO . . . . . .  33.78  ECL deviates  0.002   Reference  0.001    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  8.144     44131  0.045  0.988   14.713  15:0 ANTEISO . . . .  14.16  ECL deviates  0.002   Reference  0.001    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  8.358      1814  0.047  0.986   14.855  15:1 w6c . . . . . .   0.58  ECL deviates -0.001                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  8.574      2293  0.046  0.983   14.999  15:0 . . . . . . . .   0.73  ECL deviates -0.001   Reference -0.002    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  9.598      4068  0.049  0.973   15.627  16:0 ISO . . . . . .   1.28  ECL deviates  0.001   Reference -0.001    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  9.838      4582  0.042  0.971   15.774  16:1 w9c . . . . . .   1.44  ECL deviates -0.000                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  9.906     42748  0.051  0.970   15.815  16:1 w7c . . . . . .  13.46  ECL deviates -0.002                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 10.205      8235  0.050  0.967   15.999  16:0 . . . . . . . .   2.59  ECL deviates -0.001   Reference -0.004    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 10.915     10013  0.050  0.961   16.416  ISO 17:1 w9c . . . .   3.12  ECL deviates -0.000                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 11.279      8020  0.050  0.958   16.629  17:0 ISO . . . . . .   2.49  ECL deviates  0.000   Reference -0.002    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 11.436       838  0.053  0.956   16.722  17:0 ANTEISO . . . .   0.26  ECL deviates -0.000   Reference -0.003    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 11.553      1897  0.058  0.955   16.790  17:1 w8c . . . . . .   0.59  ECL deviates -0.002                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "*******      1297  . . .  . . .    . . .  SUMMED FEATURE 1 . .   0.42  14:1 w5t/14:1 w5c     14:1 w5c/14:1 w5t   \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "Solvent Ar  Total Area  Named Area  % Named  Total Amnt  Nbr Ref  ECL Deviation  Ref ECL Shift\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "----------  ----------  ----------  -------  ----------  -------  -------------  -------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  37515000      309634      309634   100.00      308125       12          0.003          0.003\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "_\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "                                                                                          DATA2:F90426403     06-JAN-95 16:29:38\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "ID:     3764       XANT-ARBOR-CORYL (LMG 689)                                     Date of run: 26-APR-90 20:42:59\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "Bottle: 18         SAMPLE      [AEROBE]                                           Date edited: 18-AUG-91 20:02:42\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "   RT      Area    Ar/Ht Respon     ECL           Name            %         Comment 1             Comment 2      \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "------- --------- ------ ------   ------  -------------------- ------  --------------------  --------------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  1.506  29949000  0.060  . . .    7.039  SOLVENT PEAK . . . .  . . .  < min rt                                  \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  2.894      1405  0.029  1.175    9.999  10:0 . . . . . . . .   0.73  ECL deviates -0.001   Reference  0.002    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  3.289     10442  0.032  1.142   10.606  11:0 ISO . . . . . .   5.29  ECL deviates  0.001   Reference  0.003    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  3.905       518  0.033  1.100   11.415  10:0 3OH . . . . . .   0.25  ECL deviates -0.008                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  4.233      2982  0.036  1.083   11.793  unknown 11.798 . . .   1.43  ECL deviates -0.005                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  4.504      5430  0.037  1.070   12.083  11:0 ISO 3OH . . . .   2.58  ECL deviates -0.007                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  6.098      6388  0.043  1.018   13.447  12:0 3OH . . . . . .   2.89  ECL deviates -0.008                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  6.322      1554  0.044  1.012   13.617  14:0 ISO . . . . . .   0.70  ECL deviates -0.001   Reference  0.001    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  6.824      2627  0.043  1.000   13.999  14:0 . . . . . . . .   1.17  ECL deviates -0.001   Reference  0.000    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  6.979      9425  0.046  0.997   14.103  13:0 ISO 3OH . . . .   4.17  ECL deviates -0.007                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  7.104      1200  0.046  0.994   14.186  13:0 2OH . . . . . .   0.53  ECL deviates -0.005                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  7.755     73753  0.046  0.982   14.622  15:0 ISO . . . . . .  32.16  ECL deviates  0.001   Reference  0.002    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  7.889     30801  0.047  0.979   14.712  15:0 ANTEISO . . . .  13.40  ECL deviates  0.001   Reference  0.001    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  8.099      1239  0.049  0.976   14.852  15:1 w6c . . . . . .   0.54  ECL deviates -0.004                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  8.319      2279  0.049  0.972   14.999  15:0 . . . . . . . .   0.98  ECL deviates -0.001   Reference  0.000    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  9.337      5160  0.050  0.957   15.626  16:0 ISO . . . . . .   2.19  ECL deviates  0.000   Reference  0.000    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  9.573      3318  0.039  0.953   15.771  16:1 w9c . . . . . .   1.40  ECL deviates -0.003                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  9.643     35493  0.053  0.952   15.814  16:1 w7c . . . . . .  15.01  ECL deviates -0.003                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  9.944      7863  0.050  0.948   16.000  16:0 . . . . . . . .   3.31  ECL deviates -0.000   Reference  0.000    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 10.649     11127  0.053  0.940   16.415  ISO 17:1 w9c . . . .   4.64  ECL deviates -0.001                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 11.015     11616  0.052  0.936   16.630  17:0 ISO . . . . . .   4.83  ECL deviates  0.001   Reference  0.001    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 11.170       983  0.053  0.934   16.721  17:0 ANTEISO . . . .   0.41  ECL deviates -0.001   Reference -0.001    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 11.286      2389  0.054  0.933   16.789  17:1 w8c . . . . . .   0.99  ECL deviates -0.003                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 12.974       979  0.057  0.916   17.766  18:1 w9c . . . . . .   0.40  ECL deviates -0.003                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "Solvent Ar  Total Area  Named Area  % Named  Total Amnt  Nbr Ref  ECL Deviation  Ref ECL Shift\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "----------  ----------  ----------  -------  ----------  -------  -------------  -------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  29949000      228971      228971   100.00      225207       11          0.004          0.001\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "_                                                                                         DATA2:F89B21603     06-JAN-95 16:29:38\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "ID:     1709       XANT-ARBOR-JUGLA (LMG 747)                                     Date of run: 21-NOV-89 23:44:50\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "Bottle: 13         SAMPLE      [AEROBE]\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "   RT      Area    Ar/Ht Respon     ECL           Name            %         Comment 1             Comment 2      \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "------- --------- ------ ------   ------  -------------------- ------  --------------------  --------------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  1.506  28835000  0.057  . . .    7.032  SOLVENT PEAK . . . .  . . .  < min rt                                  \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  1.666      1294  0.022  . . .    7.361   . . . . . . . . . .  . . .  < min rt                                  \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  1.857       687  0.024  . . .    7.754   . . . . . . . . . .  . . .  < min rt                                  \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  2.566       504  0.028  . . .    9.214   . . . . . . . . . .  . . .                                            \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  2.948      1197  0.028  1.165   10.000  10:0 . . . . . . . .   0.67  ECL deviates -0.000   Reference -0.002    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  3.356      8077  0.034  1.131   10.606  11:0 ISO . . . . . .   4.39  ECL deviates  0.001   Reference -0.001    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  3.998       883  0.033  1.089   11.421  10:0 3OH . . . . . .   0.46  ECL deviates -0.002                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  4.332      3857  0.034  1.071   11.795  unknown 11.798 . . .   1.98  ECL deviates -0.003                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  4.613      4840  0.035  1.058   12.087  11:0 ISO 3OH . . . .   2.46  ECL deviates -0.003                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  5.007       597  0.034  1.044   12.437  11:0 3OH . . . . . .   0.30  ECL deviates -0.004                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  5.769       538  0.035  1.019   13.095  12:0 ISO 3OH . . . .   0.26  ECL deviates -0.003                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  6.248      5161  0.040  1.007   13.452  12:0 3OH . . . . . .   2.50  ECL deviates -0.003                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  6.469      1294  0.039  1.002   13.617  14:0 ISO . . . . . .   0.62  ECL deviates -0.001   Reference -0.003    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  6.982      1936  0.041  0.990   13.999  14:0 . . . . . . . .   0.92  ECL deviates -0.001   Reference -0.003    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  7.145      7357  0.042  0.987   14.107  13:0 ISO 3OH . . . .   3.49  ECL deviates -0.003                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  7.274      1001  0.041  0.985   14.192  13:0 2OH . . . . . .   0.47  ECL deviates  0.001                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  7.929     58124  0.042  0.973   14.623  15:0 ISO . . . . . .  27.16  ECL deviates  0.002   Reference -0.000    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  8.064     34966  0.043  0.971   14.712  15:0 ANTEISO . . . .  16.30  ECL deviates  0.001   Reference -0.001    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  8.281      1680  0.043  0.967   14.855  15:1 w6c . . . . . .   0.78  ECL deviates -0.001                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  8.502      3016  0.045  0.964   15.000  15:0 . . . . . . . .   1.40  ECL deviates  0.000   Reference -0.002    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  9.533      6546  0.046  0.951   15.626  16:0 ISO . . . . . .   2.99  ECL deviates  0.000   Reference -0.002    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  9.774      3171  0.041  0.948   15.773  16:1 w9c . . . . . .   1.44  ECL deviates -0.001                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  9.845     31350  0.047  0.947   15.816  16:1 w7c . . . . . .  14.26  ECL deviates -0.001                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 10.146      6235  0.046  0.944   15.998  16:0 . . . . . . . .   2.83  ECL deviates -0.002   Reference -0.004    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 10.861     12805  0.047  0.937   16.416  ISO 17:1 w9c . . . .   5.76  ECL deviates -0.000                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 11.228     13904  0.048  0.934   16.630  17:0 ISO . . . . . .   6.23  ECL deviates  0.001   Reference -0.003    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 11.386      1886  0.049  0.933   16.722  17:0 ANTEISO . . . .   0.84  ECL deviates -0.000   Reference -0.003    \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      " 11.506      3316  0.051  0.932   16.792  17:1 w8c . . . . . .   1.48  ECL deviates -0.000                       \n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "Solvent Ar  Total Area  Named Area  % Named  Total Amnt  Nbr Ref  ECL Deviation  Ref ECL Shift\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "----------  ----------  ----------  -------  ----------  -------  -------------  -------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "  28835000      214241      213737    99.76      208252       11          0.002          0.003\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "\n",
      "\n",
      "./input_data/text_files/FAME.txt\n",
      "_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With Fileinput library\n",
    "a = [\"./input_data/text_files/example3.txt\", \n",
    "     \"./input_data/text_files/Genescan.txt\", \n",
    "     \"./input_data/text_files/FAME.txt\"]\n",
    "b = fileinput.input(a)\n",
    "for line in b:\n",
    "    print(b.filename())\n",
    "    print(line)\n",
    "b.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe0a93",
   "metadata": {},
   "source": [
    "# NOTEBOOK END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f46bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590dff9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c51ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
